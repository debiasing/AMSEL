{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from os.path import join, isdir\n",
    "from os import mkdir\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "from utils import worst_subgroup_accuracy\n",
    "\n",
    "from config import get_config\n",
    "from datasets import BiasSubset\n",
    "from datasets.utils import BalancedSampler\n",
    "from models.utils import extract_features\n",
    "from shortcut_removal.dfr import DeepFeatureReweighting\n",
    "from shortcut_removal.dfr import CustomLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# NOTE: Cell tagged as 'parameters' to allow for execution with\n",
    "# papermill/nbconvert.\n",
    "\n",
    "# Reproducibility.\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# General settings.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "# Matplotlib settings.\n",
    "mpl.rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': 14})\n",
    "mpl.rc('text', usetex=True)\n",
    "\n",
    "# Visualization settings.\n",
    "# The colors are taken from tab20.\n",
    "colors = {\n",
    "    \"ERM\": (\"#1f77b4\", \"#aec7e8\", \"--\"),\n",
    "    \"DFR\": (\"#ff7f0e\", \"#ffbb78\", \":\"),\n",
    "    \"EvA\": (\"#E377C2\", \"#F7B6D2\", \"-.\"),\n",
    "    \"AMSEL (Ours)\": (\"#2CA02C\", \"#98DF8A\", \"-\"),\n",
    "}\n",
    "\n",
    "# Experiment-specific settings.\n",
    "regularization_parameters_c = [70_000, 30_000, 10_000, 7_000, 3_000, 1_000., 700., 300., 100., 70., 30., 10., 7., 3., 1., 0.7, 0.3, 0.1, 0.07, 0.03, 0.01, 0.007, 0.003, 0.001]\n",
    "balancing_factor_step = 0.05\n",
    "balancing_factor_mode = \"default\" # 'default' vs. 'min_max'\n",
    "score_mapping = \"linear_regression\"\n",
    "score_function_name = \"Consensus\"\n",
    "if \"dataset_name\" not in globals():\n",
    "    dataset_names = [\"celeba\", \"chestx-ray14\"]\n",
    "    dataset_name = dataset_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_df(df):\n",
    "    # Introduces automatic line-breaks.\n",
    "    styler = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '120px'), (\"word-break\", \"break-all\")])])\n",
    "    display(HTML(styler.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for DFR / AMSEL Classifiers\n",
    "\n",
    "This notebook details the process of selecting the optimal inverse regularization strength, $C$, for the logistic regression classifiers used in our method. This is a necessary prerequisite for the main experiments.\n",
    "\n",
    "### Methodology\n",
    "To ensure a fair comparison and robust results, we follow the hyperparameter selection procedure from Kirichenko et al. (DFR). The core idea is to select the hyperparameter that maximizes worst-group performance.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Data Splitting:** We use the same validation data split as in the main experiment:\n",
    "    *   $\\mathcal{D}_{\\mathrm{candidate}}$ (`subset_train_dfr`): Used to **train** the classifier heads.\n",
    "    *   $\\mathcal{D}_{\\mathrm{mapping}}$ (`subset_train_score_mapping`): Used as a held-out **validation set** to evaluate the trained heads and select the best `C`.\n",
    "2.  **Model Training:** For a range of candidate values for $C$, we train classifier heads on a **fully balanced** version of $\\mathcal{D}_{\\mathrm{candidate}}$ (i.e., with balancing parameter $\\theta=1.0$). This setup is equivalent to the standard DFR method.\n",
    "3.  **Evaluation and Selection:** We evaluate each trained model on the validation set $\\mathcal{D}_{\\mathrm{mapping}}$. The value of $C$ that yields the highest **worst-group accuracy** is chosen as the optimal hyperparameter.\n",
    "\n",
    "## 1. Setup and Preliminaries\n",
    "\n",
    "This section covers the initial setup, which mirrors the main experiment notebook. We load configurations, prepare data splits, and define subgroups.\n",
    "\n",
    "### 1.1. Configuration and Parameters\n",
    "\n",
    "We begin by loading the dataset-specific configuration and defining the set of balancing parameters $\\theta$ that will be used to construct the candidate models and simulate different test distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating AMSEL for the following dataset: {dataset_name}\")\n",
    "print(f\"Evaluating the following inverse regularization strengths: {regularization_parameters_c}\")\n",
    "\n",
    "# Load config.\n",
    "dataset_config = get_config(dataset_name, root_dir=root_dir)\n",
    "\n",
    "# Prepare output folder.\n",
    "output_dir = join(\".\", \"outputs\", dataset_name)\n",
    "if not isdir(output_dir):\n",
    "    print(f\"Creating output directory: {output_dir}\")\n",
    "    mkdir(output_dir)\n",
    "\n",
    "# Extract parameters.\n",
    "balancing_factor_min = dataset_config.balancing_factor_min\n",
    "balancing_factor_max = dataset_config.balancing_factor_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if balancing_factor_mode == \"min_max\":\n",
    "    # Calculate the smallest multiple of step >= minimum.\n",
    "    start = np.ceil(balancing_factor_min / balancing_factor_step) * balancing_factor_step\n",
    "\n",
    "    # Calculate the largest multiple of step <= maximum.\n",
    "    end = np.floor(balancing_factor_max / balancing_factor_step) * balancing_factor_step\n",
    "\n",
    "    # Generate balancing factors.\n",
    "    balancing_factors = np.arange(start, end + balancing_factor_step / 2, balancing_factor_step)\n",
    "elif balancing_factor_mode == \"default\":\n",
    "    balancing_factors = np.round(np.linspace(0, 1., 21), 2) # Round due some numbers like 0.6 becoming 0.6.....1 (precision errors)\n",
    "    balancing_factor_min, balancing_factor_max = 0, 1\n",
    "else:\n",
    "    raise ValueError(\"Invalid balancing factor creation mode specified!\")\n",
    "\n",
    "assert 0 in balancing_factors and 1 in balancing_factors, \"Make sure that you balancing factors contain both theta=0 and theta=1.\"\n",
    "unbiased_model_idx = np.where(balancing_factors == 1.0)[0][0]\n",
    "print(\"Evaluating balancing factors:\", balancing_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_factor_to_index(balancing_factor: float, clip: bool = False) -> Union[float, int]:\n",
    "    # While we could cast the index to int because indices have to be integers,\n",
    "    # we explicitly choose to leave this to the user to ensure invertability.\n",
    "    # [min, max] -> [0,1].\n",
    "    balancing_factor = (balancing_factor - np.min(balancing_factors)) / (np.max(balancing_factors) - np.min(balancing_factors))\n",
    "\n",
    "    # [0,1] -> [0, len(balancing_factors) - 1].\n",
    "    balancing_factor_idx = balancing_factor * (len(balancing_factors) - 1)\n",
    "\n",
    "    if clip:\n",
    "         balancing_factor_idx = np.clip(balancing_factor_idx, a_min=0, a_max=len(balancing_factors) - 1)\n",
    "\n",
    "    return balancing_factor_idx\n",
    "\n",
    "def index_to_balancing_factor(balancing_factor_idx: int, clip: bool = False) -> float:\n",
    "    # [0, len(balancing_factors) - 1] -> [0,1].\n",
    "    balancing_factor = balancing_factor_idx / (len(balancing_factors) - 1)\n",
    "\n",
    "    # [0,1] -> [min, max].\n",
    "    balancing_factor = balancing_factor * (np.max(balancing_factors) - np.min(balancing_factors)) + np.min(balancing_factors)\n",
    "    \n",
    "    if clip:\n",
    "        balancing_factor = np.clip(balancing_factor, a_min=np.min(balancing_factors), a_max=np.max(balancing_factors))\n",
    "                                   \n",
    "    return balancing_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Loading and Preparation\n",
    "\n",
    "We load the validation dataset and split it into two disjoint halves. This setup is identical to the main experiment, but the roles of the splits are adapted for hyperparameter tuning.\n",
    "-   `subset_train_dfr` ($\\mathcal{D}_{\\mathrm{candidate}}$): The **training set** for the classifier heads.\n",
    "-   `subset_train_score_mapping` ($\\mathcal{D}_{\\mathrm{mapping}}$): The held-out **validation set** for selecting the best `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, dataset_test = dataset_config.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we have to shuffle the indices because the validation dataset might\n",
    "# be sorted. However, because we pass a seed, the shuffling is fully\n",
    "# reproducible.\n",
    "indices = list(range(len(dataset_val)))\n",
    "random_generator = np.random.default_rng(seed)\n",
    "random_generator.shuffle(indices)\n",
    "subset_train_dfr = BiasSubset(dataset_val, indices=indices[:len(indices)//2])\n",
    "subset_train_score_mapping = BiasSubset(dataset_val, indices=indices[len(indices)//2:])\n",
    "subset_test = dataset_test\n",
    "\n",
    "print(\"#Samples Train DFR:\\t\\t\", len(subset_train_dfr))\n",
    "print(\"#Samples Train Score Mapping:\\t\", len(subset_train_score_mapping))\n",
    "print(\"#Samples Test:\\t\\t\\t\", len(subset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels.\n",
    "labels_train_dfr = np.array([subset_train_dfr.label(idx) for idx in range(len(subset_train_dfr))])\n",
    "labels_train_score_mapping = np.array([subset_train_score_mapping.label(idx) for idx in range(len(subset_train_score_mapping))])\n",
    "labels_test = np.array([subset_test.label(idx) for idx in range(len(subset_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader.\n",
    "dataloader_train_dfr = DataLoader(\n",
    "    subset_train_dfr,\n",
    "    batch_size=100,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_train_score_mapping = DataLoader(\n",
    "    subset_train_score_mapping,\n",
    "    batch_size=100,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    subset_test,\n",
    "    batch_size=100,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Model and Feature Extractor\n",
    "\n",
    "We load a standard ERM model pre-trained on the biased training set $\\mathcal{D}$.\n",
    "The feature extractor $e$ of this model will be fixed and used to generate feature representations for all subsequent steps.\n",
    "We consider multiple ERM models (trained with different seeds) to report mean and standard deviation over our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = dataset_config.ModelClass\n",
    "base_models = dataset_config.get_base_models()\n",
    "print(f\"Considering the following base models: {base_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Subgroup Identification\n",
    "\n",
    "As defined in the paper, we formalize bias as an imbalance between subgroups $\\mathcal{G}$, where each subgroup is a combination of a class label and a bias attribute. We identify these subgroups for each data split, as the subgroup labels are required for creating the re-weighted datasets $\\mathcal{D}_\\theta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubgroupIdentifier = dataset_config.SubgroupIdentifierClass\n",
    "print(f\"Using the following subgroup identification method: {SubgroupIdentifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_identifier = SubgroupIdentifier()\n",
    "group_ids_train_dfr, n_subgroups = subgroup_identifier.identify_subgroups(None, subset_train_dfr, device)\n",
    "group_ids_train_score_mapping, _ = subgroup_identifier.identify_subgroups(None, subset_train_score_mapping, device)\n",
    "group_ids_test, _ = subgroup_identifier.identify_subgroups(None, subset_test, device)\n",
    "group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test = [z.cpu().detach().numpy() for z in (group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test)]\n",
    "\n",
    "# Remove empty groups if desired (i.e., re-ordering such that empty groups are\n",
    "# listed last and then discarding them).\n",
    "if dataset_config.subgroup_postprocessor:\n",
    "    n_subgroups, group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test = dataset_config.subgroup_postprocessor(group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test)\n",
    "\n",
    "# Are the subgroups balanced?\n",
    "print(\"Number of subgroups: \", n_subgroups)\n",
    "print(\"Subgroup balancing (train DFR):\", np.unique(group_ids_train_dfr, return_counts=True)[1])\n",
    "print(\"Subgroup balancing (train score mapping):\", np.unique(group_ids_train_score_mapping, return_counts=True)[1])\n",
    "print(\"Subgroup balancing (test subset):\", np.unique(group_ids_test, return_counts=True)[1])\n",
    "\n",
    "# Test for missing subgroups.\n",
    "# We expect that each subset contains all subgroups when performing the balanced\n",
    "# subsampling. Alternatively, one could adapt the BalancedSampler such that it\n",
    "# allows subgroups not to be present.\n",
    "assert len(set(group_ids_train_dfr)) == n_subgroups, \"No samples of the following subgroup(s) in subset_train_dfr: \" + str(set(range(n_subgroups)).difference(set(group_ids_train_dfr))) \n",
    "assert len(set(group_ids_train_score_mapping)) == n_subgroups, \"No samples of the following subgroup(s) in subset_train_score_mapping: \" + str(set(range(n_subgroups)).difference(set(group_ids_train_score_mapping)))\n",
    "assert len(set(group_ids_test)) == n_subgroups, \"No samples of the following subgroup(s) in subset_test: \" + str(set(range(n_subgroups)).difference(set(group_ids_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Simulating Test Distributions with Varying Bias\n",
    "\n",
    "To evaluate the robustness of our method, we simulate a range of test distributions with varying degrees of spurious correlation. We achieve this by re-sampling the validation and test sets according to the parameterized distribution $P_\\theta(Y, B) = \\theta \\, P_{\\mathrm{bal}}(Y,B) + (1-\\theta) \\, P_{\\mathrm{orig}}(Y,B)$, where $\\theta=0$ corresponds to the original biased distribution and $\\theta=1$ corresponds to a perfectly balanced one. The `BalancedSampler` generates subsets of indices that approximate these target distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset subsamples with the desired balancing factors.\n",
    "# Note that since we set the seed, all models are evaluated on the \n",
    "# same subsets (per balancing factor).\n",
    "balancing_mode = \"exact_under\"\n",
    "subset_indices_per_balancing_factor_train_dfr = [\n",
    "    list(\n",
    "        BalancedSampler(\n",
    "            subset_train_dfr,\n",
    "            mode=balancing_mode,\n",
    "            balancing_factor=balancing_factor,\n",
    "            labels=torch.tensor(group_ids_train_dfr),\n",
    "            seed=seed\n",
    "        )\n",
    "    )\n",
    "    for balancing_factor in balancing_factors\n",
    "]\n",
    "\n",
    "subset_indices_per_balancing_factor_train_score_mapping = [\n",
    "    list(\n",
    "        BalancedSampler(\n",
    "            subset_train_score_mapping,\n",
    "            mode=balancing_mode,\n",
    "            balancing_factor=balancing_factor,\n",
    "            labels=torch.tensor(group_ids_train_score_mapping),\n",
    "            seed=seed\n",
    "        )\n",
    "    )\n",
    "    for balancing_factor in balancing_factors\n",
    "]\n",
    "\n",
    "subset_indices_per_balancing_factor_test = [\n",
    "    list(\n",
    "        BalancedSampler(\n",
    "            subset_test,\n",
    "            mode=balancing_mode,\n",
    "            balancing_factor=balancing_factor,\n",
    "            labels=torch.tensor(group_ids_test),\n",
    "            seed=seed\n",
    "        )\n",
    "    )\n",
    "    for balancing_factor in balancing_factors\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below visualizes the effect of the balancing parameter $\\theta$ on the subgroup composition of the resulting test subsets. As $\\theta$ increases from 0 to 1, the proportions of all subgroups converge to be uniform, effectively removing the spurious correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5.0,3.2))\n",
    "\n",
    "for subgroup_id in np.unique(group_ids_test):\n",
    "    counts = [np.sum(group_ids_test[indices] == subgroup_id) / len(indices) for indices in subset_indices_per_balancing_factor_test]\n",
    "    plt.plot(balancing_factors, counts, label=\"Subgroup \" + str(subgroup_id + 1))\n",
    "\n",
    "    # Calculate intercept with the x-axis.\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(balancing_factors.reshape(-1, 1), counts)\n",
    "    slope = lm.coef_[0]\n",
    "    intercept = lm.intercept_\n",
    "    x_intercept = -intercept / slope\n",
    "    print(\"x-intercept for subgroup \" + str(subgroup_id + 1) + \": \" + str(x_intercept))\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(r\"Balancing Parameter $\\theta$ of the Test Distribution\")\n",
    "plt.ylabel(\"Percentage of Samples\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Classifier Heads for Hyperparameter Search\n",
    "\n",
    "This section implements the training loop for the hyperparameter search.\n",
    "\n",
    "**Key Difference from Main Experiment:**\n",
    "-   We iterate through the list of candidate regularization strengths `C`.\n",
    "-   For each `C`, we train a **single** classifier head.\n",
    "-   This head is trained on a **fully balanced** dataset (`balancing_factor=1.0`), making it equivalent to a standard DFR model.\n",
    "\n",
    "After training, we generate predictions for each model on the held-out validation set (`subset_train_score_mapping`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility.\n",
    "# We set the seed explicitly so that this cell is re-producible even when\n",
    "# reexecuted (standalone reproducibility).\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Train models and extract outputs.\n",
    "tasks = {}\n",
    "for base_model in base_models:\n",
    "    print(\n",
    "        \"Generating reweighted DFR models and extracting model outputs for DFR models with feature backbone: \"\n",
    "        + str(base_model)\n",
    "    )\n",
    "\n",
    "    # Load corresponding ERM model.\n",
    "    model = Model(root=root_dir, model=base_model, download=False).to(device)\n",
    "\n",
    "    # Extract training and test features.\n",
    "    # The resulting feature vectors have shape [n_samples, n_feature_dims].\n",
    "    with torch.no_grad():\n",
    "        feature_extractor = model.get_feature_extractor().eval()\n",
    "        embeddings, labels = extract_features(\n",
    "            feature_extractor=feature_extractor,\n",
    "            dataloader=dataloader_train_dfr,\n",
    "            extract_labels=True,\n",
    "            device=device,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        X_train_dfr, y_train_dfr = [x.detach().cpu().numpy() for x in (embeddings, labels)]\n",
    "\n",
    "        embeddings, labels = extract_features(\n",
    "            feature_extractor=feature_extractor,\n",
    "            dataloader=dataloader_train_score_mapping,\n",
    "            extract_labels=True,\n",
    "            device=device,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        X_train_score_mapping, y_train_score_mapping = [x.detach().cpu().numpy() for x in (embeddings, labels)]\n",
    "\n",
    "    # Define and train DFR models.\n",
    "    dfr_models = []\n",
    "    for regularization_parameter_c in tqdm(\n",
    "        regularization_parameters_c, desc=\"Train DFR models\"\n",
    "    ):\n",
    "        # Select appropriate solver for the logistic regression.\n",
    "        if len(subset_train_score_mapping.class_labels) == 2:\n",
    "            clf = CustomLogisticRegression(\n",
    "                num_retrains=20,\n",
    "                balancing_factor=1.0,\n",
    "                C=regularization_parameter_c,\n",
    "                penalty=\"l1\",\n",
    "                solver=\"liblinear\",\n",
    "                random_state=seed,\n",
    "                class_weight=\"balanced\",\n",
    "                # verbose=1,\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Multiclass problem, selecting different solver and penalty because this is not supported by liblinear!\")\n",
    "            clf = CustomLogisticRegression(\n",
    "                num_retrains=20,\n",
    "                balancing_factor=1.0,\n",
    "                C=regularization_parameter_c,\n",
    "                solver=\"lbfgs\",\n",
    "                n_jobs=-1,\n",
    "                # penalty=\"l1\",\n",
    "                random_state=seed,\n",
    "                class_weight=\"balanced\",\n",
    "                # verbose=1,\n",
    "            )   \n",
    "\n",
    "        dfr = DeepFeatureReweighting(classifier=clf)\n",
    "        dfr_model = dfr._remove_shortcut(\n",
    "            model=model,\n",
    "            feature_extractor=feature_extractor,\n",
    "            X=X_train_dfr,\n",
    "            y=y_train_dfr,\n",
    "            group_ids=group_ids_train_dfr,\n",
    "            device=device,\n",
    "        )\n",
    "        dfr_models.append(dfr_model.to(\"cpu\"))\n",
    "\n",
    "    # Extract model predictions on the evaluation subset.\n",
    "    outputs_train_score_mapping = []\n",
    "    for model in tqdm(dfr_models, desc=\"Extract predicted probabilities for subset_train_score_mapping\"):\n",
    "        model = model.to(device)\n",
    "        x = np.copy(X_train_score_mapping)\n",
    "\n",
    "        # Preprocessing and classification.\n",
    "        if model.preprocess:\n",
    "            x = model.scaler.transform(x)\n",
    "        x = model.classifier.predict_proba(x)\n",
    "        outputs_train_score_mapping.append(x)\n",
    "    outputs_train_score_mapping = np.array(outputs_train_score_mapping)\n",
    "\n",
    "    if outputs_train_score_mapping.shape[2] == 2:\n",
    "        # Binary problem.\n",
    "        outputs_train_score_mapping = np.column_stack([x[:, 1] for x in outputs_train_score_mapping])\n",
    "    else:\n",
    "        # Multic-class problem.\n",
    "        # Permute axes to [n_samples, n_regularization_parameters_c, n_classes].\n",
    "        outputs_train_score_mapping = np.array(outputs_train_score_mapping).transpose((1, 0, 2))\n",
    "\n",
    "        # Flatten the logits per balancing factor into a one-dimensional feature\n",
    "        # vector.\n",
    "        outputs_train_score_mapping = np.row_stack([x.flatten() for x in outputs_train_score_mapping])\n",
    "\n",
    "    tasks[base_model] = {\n",
    "        \"X_train_dfr\": X_train_dfr,\n",
    "        \"y_train_dfr\": y_train_dfr,\n",
    "        \"X_train_score_mapping\": X_train_score_mapping,\n",
    "        \"y_train_score_mapping\": y_train_score_mapping,\n",
    "        \"outputs_train_score_mapping\": outputs_train_score_mapping,\n",
    "    }\n",
    "\n",
    "print(\"Extracted model outputs of shape \" + str(outputs_train_score_mapping.shape) + \" for evaluating the DFR models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper functions convert the raw probability outputs from the classifiers into class predictions, handling both binary and multi-class cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuncClassifier:\n",
    "    def __init__(self, func, n_classes):\n",
    "        self.func = func\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # If binary, simply apply and check whether >= 0.\n",
    "        # Otherwise, we first group by class labels and then apply to the\n",
    "        # grouped variant. Afterwards, we predict via argmax.\n",
    "        X = self.predict_proba(X)\n",
    "        if self.n_classes == 2:\n",
    "            return (X >= 0.5).astype(int)\n",
    "        else:\n",
    "            # Predict via argmax.\n",
    "            return np.argmax(X, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # For binary classification, we directly apply the function because the\n",
    "        # data only contains a single logit per regularization parameter. For\n",
    "        # multi-class classification, we first group by class labels and then\n",
    "        # apply to the grouped variant. Afterwards, we predict via argmax.\n",
    "        if self.n_classes == 2:\n",
    "            return np.apply_along_axis(self.func, 1, X)\n",
    "        else:\n",
    "            # Reshape from [n_samples, n_regularization_parameters_c x n_classes] to \n",
    "            # [n_samples, n_regularization_parameters_c, n_classes]\n",
    "            assert X.shape[1] % self.n_classes == 0, \"The second data dimension is expected to be n_regularization_parameters_c x n_classes but your dimension is not divisible by the specified number of classes: shape \" + str(X.shape) + \" for n_classes \" + str(self.n_classes)\n",
    "            X = np.array(np.split(X, X.shape[1] // self.n_classes, axis=1))\n",
    "            X = X.transpose((1,0,2))\n",
    "\n",
    "            # Apply function.\n",
    "            return np.apply_along_axis(self.func, 1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predictions(X: np.ndarray, n_classes: int) -> np.ndarray:\n",
    "    if n_classes == 2:\n",
    "        # In the case of a binary problem, we only consider one logit (namely\n",
    "        # that of class 1). Thus, we can perform a simple thresholding to\n",
    "        # generate class predictions.\n",
    "        predictions = (X >= 0.5).astype(int)\n",
    "    else:\n",
    "        # For multi-class problems, we use a small trick: The FuncClassifier\n",
    "        # splits the one-dimensional feature vectors back into the outputs of\n",
    "        # each individual classifier. Then, we only have to apply np.argmax to\n",
    "        # the extracted probabilites.\n",
    "        clf = FuncClassifier((lambda x: x), n_classes=n_classes)\n",
    "        predictions = clf.predict_proba(X)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test dataset.\n",
    "dfr_predictions = []\n",
    "for i, base_model in enumerate(base_models):\n",
    "    # Load features and targets.\n",
    "    outputs_train_score_mapping, y_train_score_mapping = [tasks[base_model][z] for z in [\"outputs_train_score_mapping\", \"y_train_score_mapping\"]]\n",
    "\n",
    "    # Generate predictions for all DFR models.\n",
    "    p = extract_predictions(\n",
    "        outputs_train_score_mapping, n_classes=len(subset_train_score_mapping.class_labels)\n",
    "    )\n",
    "    dfr_predictions.append(p)\n",
    "dfr_predictions = np.array(\n",
    "    dfr_predictions\n",
    ")  # Shape: [n_base_models, n_samples, n_regularization_parameters_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation and Selection\n",
    "\n",
    "We now evaluate the performance of each trained model on the held-out validation set (`subset_train_score_mapping`) and select the best `C`.\n",
    "\n",
    "### 3.1. Baseline: ERM Performance\n",
    "\n",
    "As a point of reference, we first evaluate the standard ERM model on the validation set. This shows the performance of a model with no post-hoc debiasing on this specific data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erm_predictions = []\n",
    "for base_model in base_models:\n",
    "    print(\n",
    "        \"Generating predictions for ERM model: \"\n",
    "        + str(base_model)\n",
    "    )\n",
    "\n",
    "    # Load corresponding ERM model.\n",
    "    model = Model(root=root_dir, model=base_model, download=False).to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    # Conversion from model outputs to class label predictions.\n",
    "    # We could also define this function once for all base models.\n",
    "    if list(model.modules())[-1].out_features != 1:\n",
    "        model_outputs_to_preds_transform = Lambda(lambda x: torch.argmax(x, dim=1))\n",
    "    else:\n",
    "        model_outputs_to_preds_transform = Lambda(lambda x: (x >= 0.5).long())\n",
    "\n",
    "    # Extract predictions.\n",
    "    with torch.no_grad():\n",
    "        pred = torch.cat([model_outputs_to_preds_transform(model(x[0].to(device))).cpu()\n",
    "                                    for x in tqdm(dataloader_train_score_mapping)])\n",
    "        erm_predictions.append(pred)\n",
    "del pred\n",
    "model = model.to(\"cpu\")\n",
    "erm_predictions = np.array(erm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erm_accuracies = np.array([accuracy_score(y_train_score_mapping, p) for p in erm_predictions])\n",
    "erm_wsg_accuracies = np.array(\n",
    "    [worst_subgroup_accuracy(y_train_score_mapping, p, group_ids_train_score_mapping) for p in erm_predictions]\n",
    ")\n",
    "\n",
    "print(\"ERM Statistics on Evaluation Subset:\\n\")\n",
    "print(\"Accuracy (Avg):\\t\", np.mean(erm_accuracies))\n",
    "print(\"Accuracy (Std):\\t\", np.std(erm_accuracies))\n",
    "print(\"WSG (Avg):\\t\", np.mean(erm_wsg_accuracies))\n",
    "print(\"WSG (Std):\\t\", np.std(erm_wsg_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. DFR Model Performance vs. Regularization Strength\n",
    "\n",
    "We now compute the overall accuracy and worst-subgroup accuracy for each of the DFR models trained with different regularization strengths `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_accuracies = np.array(\n",
    "    [\n",
    "        [accuracy_score(y_train_score_mapping, p) for p in predictions.T]\n",
    "        for predictions in dfr_predictions\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Worst Subgroup Accuracy.\n",
    "dfr_wsg_accuracies = np.array(\n",
    "    [\n",
    "        [worst_subgroup_accuracy(y_train_score_mapping, p, group_ids_train_score_mapping) for p in predictions_per_regularization_parameter.T]\n",
    "        for predictions_per_regularization_parameter in dfr_predictions\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Results and Selection\n",
    "\n",
    "The table below summarizes the performance for each value of `C`. Following the DFR methodology, we select the value of `C` that **maximizes the Worst-Group Accuracy**. This ensures the chosen model is robust and performs well even on the most challenging, bias-conflicting data subgroups. The best-performing value for each metric is highlighted in bold.\n",
    "\n",
    "We display the results both as a markdown table as well as a TeX table (simplifying data export)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Inverse Regularization Strength c\" : regularization_parameters_c,\n",
    "        \"Accuracy (Avg)\" : np.mean(dfr_accuracies, axis=0),\n",
    "        \"Accuracy (Std)\" : np.std(dfr_accuracies, axis=0),\n",
    "        \"WSG (Avg)\" : np.mean(dfr_wsg_accuracies, axis=0),\n",
    "        \"WSG (Std)\" : np.std(dfr_wsg_accuracies, axis=0),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dataset_name)\n",
    "print(np.mean(dfr_wsg_accuracies, axis=0)) # Printing because this is the column we choose the hyperparameter on\n",
    "print(display_df(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_significance(m, s, phantom_std=True):\n",
    "    if phantom_std:\n",
    "        return f\"{m:>6.2%} ± {s:>{6}.2%}\".replace(\" \", \"\\\\phantom{0}\").replace(\"%\", \"\")\n",
    "    else:\n",
    "        return f\"{m:>6.2%} ± {s:>{5}.2%}\".replace(\" \", \"\\\\phantom{0}\").replace(\"%\", \"\")\n",
    "\n",
    "def scientific_notation(number, sig_fig=0):\n",
    "    ret_string = \"{0:.{1:d}e}\".format(number, sig_fig)\n",
    "    a, b = ret_string.split(\"e\")\n",
    "    # remove leading \"+\" and strip leading zeros\n",
    "    b = int(b)\n",
    "    return a + \" * 10^\" + str(b)\n",
    "\n",
    "def scientific_notation(x, digits=0):\n",
    "    enot = \"{0:.{1:d}e}\".format(x, digits)\n",
    "    mantisse, exponent = enot.split(\"e\")\n",
    "    exponent = int(exponent)\n",
    "\n",
    "    return mantisse + \" \\\\cdot 10^{\" + str(exponent) + \"}\"\n",
    "\n",
    "print(scientific_notation(0.70))\n",
    "\n",
    "# Find max values.\n",
    "# We later need this to make the best values bold.\n",
    "max_acc = df[\"Accuracy (Avg)\"].max()\n",
    "max_wsg = df[\"WSG (Avg)\"].max()\n",
    "\n",
    "# Export DataFrame to LaTeX.\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        # \"Model\": \"DFR Model\",\n",
    "        # \"Inverse Regularization Strength c\": df[\"Inverse Regularization Strength c\"].apply(lambda x: f\"{x:>{9}.3f}\".replace(\" \", \"\\\\phantom{0}\")),\n",
    "        \"Inverse Regularization Strength c\": df[\"Inverse Regularization Strength c\"].apply(lambda x: \"$\" + scientific_notation(x) + \"$\"),\n",
    "        \"Accuracy (%, $\\\\uparrow$)\": [format_significance(m, s, phantom_std=(df[\"Accuracy (Std)\"] >= 0.1).any()) if m != max_acc else r\"\\textbf{\" + format_significance(m, s, phantom_std=(df[\"Accuracy (Std)\"] >= 0.1).any()) + \"}\" for m, s in zip(df[\"Accuracy (Avg)\"], df[\"Accuracy (Std)\"])],\n",
    "        \"Worst-Group Accuracy (%, $\\\\uparrow$)\": [format_significance(m, s, phantom_std=(df[\"WSG (Std)\"] >= 0.1).any()) if m != max_wsg else r\"\\textbf{\" + format_significance(m, s, phantom_std=(df[\"WSG (Std)\"] >= 0.1).any()) + \"}\" for m, s in zip(df[\"WSG (Avg)\"], df[\"WSG (Std)\"])],\n",
    "    }\n",
    ")\n",
    "# raise ValueError(\"Invalid rounding of regularization strength.\")\n",
    "print(df.to_latex(index=False).replace(\"%\", \"\\\\%\"))\n",
    "with open(join(\".\", \"outputs\", dataset_name, dataset_name + \" - Regularization Strength.tex\"), \"w\") as f:\n",
    "    f.write(df.to_latex(index=False).replace(\"%\", \"\\\\%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Visualization of Results\n",
    "\n",
    "The plot below visualizes the performance metrics as a function of the inverse regularization strength `C` (on a log scale). This provides a clear view of the impact of regularization and helps confirm that the chosen value for `C` is in a stable and effective range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyle = {\n",
    "    \"Global Accuracy (Avg)\": \"solid\",\n",
    "    \"Balanced Accuracy (Avg)\": \"dotted\",\n",
    "    \"Worst Subgroup Accuracy (Avg)\": \"dashed\"\n",
    "}\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 1. ERM performance.\n",
    "# Global Accuracy.\n",
    "y = erm_accuracies\n",
    "mean = np.mean(y).repeat(len(regularization_parameters_c))\n",
    "std = np.std(y).repeat(len(regularization_parameters_c))\n",
    "plt.plot(regularization_parameters_c, mean, label=\"Accuracy (ERM)\", color=\"C0\", linestyle=linestyle[\"Global Accuracy (Avg)\"])\n",
    "plt.fill_between(regularization_parameters_c, mean+std, mean-std, facecolor=\"C0\", alpha=0.3)\n",
    "\n",
    "# Worst Subgroup Accuracy.\n",
    "y = erm_wsg_accuracies\n",
    "mean = np.mean(y).repeat(len(regularization_parameters_c))\n",
    "std = np.std(y).repeat(len(regularization_parameters_c))\n",
    "plt.plot(regularization_parameters_c, mean, label=\"Worst-Subgroup Accuracy (ERM)\", color=\"C0\", linestyle=linestyle[\"Worst Subgroup Accuracy (Avg)\"])\n",
    "plt.fill_between(regularization_parameters_c, mean+std, mean-std, facecolor=\"C0\", alpha=0.3)\n",
    "\n",
    "# 2. DFR performance.\n",
    "# Global Accuracy.\n",
    "y = dfr_accuracies\n",
    "mean = np.mean(y, axis=0)\n",
    "std = np.std(y, axis=0)\n",
    "plt.plot(regularization_parameters_c, mean, label=\"Accuracy (DFR)\", color=\"orange\", linestyle=linestyle[\"Global Accuracy (Avg)\"])\n",
    "plt.fill_between(regularization_parameters_c, mean+std, mean-std, facecolor=\"orange\", alpha=0.3)\n",
    "\n",
    "# Worst Subgroup Accuracy.\n",
    "y = dfr_wsg_accuracies\n",
    "mean = np.mean(y, axis=0)\n",
    "std = np.std(y, axis=0)\n",
    "plt.plot(regularization_parameters_c, mean, label=\"Worst-Subgroup Accuracy (DFR)\", color=\"orange\", linestyle=linestyle[\"Worst Subgroup Accuracy (Avg)\"])\n",
    "plt.fill_between(regularization_parameters_c, mean+std, mean-std, facecolor=\"orange\", alpha=0.3)\n",
    "\n",
    "plt.ylim(0., 1.)\n",
    "plt.xscale('log')\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "plt.xlabel(r\"Inverse Regularization Strength $c$\")\n",
    "plt.ylabel(\"Performance Score\")\n",
    "plt.title(\"Performance Comparison Between Inverse Regularization Strengths\\n[Subset of Validation Dataset - Biased Distribution - \" + dataset_name + \"]\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
