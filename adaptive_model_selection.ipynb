{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from os.path import join, isdir, isfile\n",
    "from os import mkdir\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import dill as pickle\n",
    "from datetime import datetime\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, auc\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from utils import worst_subgroup_accuracy, subgroup_accuracy\n",
    "\n",
    "from config import get_config\n",
    "from datasets import BiasSubset\n",
    "from datasets.utils import BalancedSampler\n",
    "from models.utils import extract_features\n",
    "from shortcut_removal.dfr import DeepFeatureReweighting\n",
    "from shortcut_removal.dfr import CustomLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# NOTE: Cell tagged as 'parameters' to allow for execution with\n",
    "# papermill/nbconvert.\n",
    "\n",
    "# Reproducibility.\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# General settings.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "# Matplotlib settings.\n",
    "mpl.rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': 14})\n",
    "mpl.rc('text', usetex=True)\n",
    "\n",
    "# Visualization settings.\n",
    "# The colors are taken from tab20.\n",
    "colors = {\n",
    "    \"ERM\": (\"#1f77b4\", \"#aec7e8\", \"--\"),\n",
    "    \"DFR\": (\"#ff7f0e\", \"#ffbb78\", \":\"),\n",
    "    \"EvA\": (\"#E377C2\", \"#F7B6D2\", \"-.\"),\n",
    "    \"AMSEL (Ours)\": (\"#2CA02C\", \"#98DF8A\", \"-\"),\n",
    "}\n",
    "\n",
    "# Experiment-specific settings.\n",
    "balancing_factor_step = 0.05\n",
    "balancing_factor_mode = \"default\" # 'default' vs. 'min_max'\n",
    "score_mapping = \"linear_regression\"\n",
    "score_function_name = \"Consensus\"\n",
    "if \"dataset_name\" not in globals():\n",
    "    dataset_names = [\"celeba\", \"chestx-ray14\"]\n",
    "    dataset_name = dataset_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_df(df):\n",
    "    # Introduces automatic line-breaks.\n",
    "    styler = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '120px'), (\"word-break\", \"break-all\")])])\n",
    "    display(HTML(styler.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for Adaptive Model Selection (AMSEL)\n",
    "\n",
    "This notebook implements and evaluates the **Adaptive Model Selection (AMSEL)** method, as described in the paper. AMSEL addresses the problem of robust generalization under distribution shifts caused by varying degrees of spurious correlation at test time.\n",
    "\n",
    "The method consists of three main steps:\n",
    "1.  **Candidate Model Construction:** We train a family of lightweight classifier heads $\\{h_\\theta\\}$ on a fixed feature extractor $e$. Each head is trained on a dataset $\\mathcal{D}_\\theta$ with a specific degree of spurious correlation, controlled by a balancing parameter $\\theta$.\n",
    "2.  **Estimation of Test Distribution Characteristics:** We learn a mapping $M$ from a score function $s(\\cdot)$, which measures statistical properties of the input data, to the corresponding balancing parameter $\\theta$.\n",
    "3.  **Adaptive Inference:** At test time, we compute the score on the test data, use the mapping $M$ to estimate the test balancing parameter $\\theta_{\\mathrm{test}}$, and select the corresponding model $m_{\\theta_{\\mathrm{test}}} = h_{\\theta_{\\mathrm{test}}} \\circ e$ for prediction.\n",
    "\n",
    "This notebook details the implementation of this pipeline and evaluates its performance against several baselines on test sets with varying degrees of spurious correlation.\n",
    "\n",
    "## 1. Setup and Preliminaries\n",
    "\n",
    "This section covers the initial setup, including loading configurations, preparing datasets, and defining subgroups, which are essential for the subsequent steps.\n",
    "\n",
    "### 1.1. Configuration and Parameters\n",
    "\n",
    "We begin by loading the dataset-specific configuration and defining the set of balancing parameters $\\theta$ that will be used to construct the candidate models and simulate different test distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating AMSEL for the following dataset: {dataset_name}\")\n",
    "\n",
    "# Load config.\n",
    "dataset_config = get_config(dataset_name, root_dir=root_dir)\n",
    "\n",
    "# Prepare output folder.\n",
    "output_dir = join(\".\", \"outputs\", dataset_name)\n",
    "if not isdir(output_dir):\n",
    "    print(f\"Creating output directory: {output_dir}\")\n",
    "    mkdir(output_dir)\n",
    "\n",
    "# Extract parameters.\n",
    "regularization_parameter_c = dataset_config.regularization_parameter_c\n",
    "balancing_factor_min = dataset_config.balancing_factor_min\n",
    "balancing_factor_max = dataset_config.balancing_factor_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if balancing_factor_mode == \"min_max\":\n",
    "    # Calculate the smallest multiple of step >= minimum.\n",
    "    start = np.ceil(balancing_factor_min / balancing_factor_step) * balancing_factor_step\n",
    "\n",
    "    # Calculate the largest multiple of step <= maximum.\n",
    "    end = np.floor(balancing_factor_max / balancing_factor_step) * balancing_factor_step\n",
    "\n",
    "    # Generate balancing factors.\n",
    "    balancing_factors = np.arange(start, end + balancing_factor_step / 2, balancing_factor_step)\n",
    "elif balancing_factor_mode == \"default\":\n",
    "    balancing_factors = np.round(np.linspace(0, 1., 21), 2) # Round due some numbers like 0.6 becoming 0.6.....1 (precision errors)\n",
    "    balancing_factor_min, balancing_factor_max = 0, 1\n",
    "else:\n",
    "    raise ValueError(\"Invalid balancing factor creation mode specified!\")\n",
    "\n",
    "assert 0 in balancing_factors and 1 in balancing_factors, \"Make sure that you balancing factors contain both theta=0 and theta=1.\"\n",
    "unbiased_model_idx = np.where(balancing_factors == 1.0)[0][0]\n",
    "print(\"Evaluating balancing factors:\", balancing_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_factor_to_index(balancing_factor: float, clip: bool = False) -> Union[float, int]:\n",
    "    # While we could cast the index to int because indices have to be integers,\n",
    "    # we explicitly choose to leave this to the user to ensure invertability.\n",
    "    # [min, max] -> [0,1].\n",
    "    balancing_factor = (balancing_factor - np.min(balancing_factors)) / (np.max(balancing_factors) - np.min(balancing_factors))\n",
    "\n",
    "    # [0,1] -> [0, len(balancing_factors) - 1].\n",
    "    balancing_factor_idx = balancing_factor * (len(balancing_factors) - 1)\n",
    "\n",
    "    if clip:\n",
    "         balancing_factor_idx = np.clip(balancing_factor_idx, a_min=0, a_max=len(balancing_factors) - 1)\n",
    "\n",
    "    return balancing_factor_idx\n",
    "\n",
    "def index_to_balancing_factor(balancing_factor_idx: int, clip: bool = False) -> float:\n",
    "    # [0, len(balancing_factors) - 1] -> [0,1].\n",
    "    balancing_factor = balancing_factor_idx / (len(balancing_factors) - 1)\n",
    "\n",
    "    # [0,1] -> [min, max].\n",
    "    balancing_factor = balancing_factor * (np.max(balancing_factors) - np.min(balancing_factors)) + np.min(balancing_factors)\n",
    "    \n",
    "    if clip:\n",
    "        balancing_factor = np.clip(balancing_factor, a_min=np.min(balancing_factors), a_max=np.max(balancing_factors))\n",
    "                                   \n",
    "    return balancing_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Loading and Preparation\n",
    "\n",
    "We load the validation and test datasets. Following the methodology, we split the validation set into two disjoint halves:\n",
    "-   $\\mathcal{D}_{\\mathrm{candidate}}$ (`subset_train_dfr`): Used to train the family of candidate classifier heads $\\{h_\\theta\\}$.\n",
    "-   $\\mathcal{D}_{\\mathrm{mapping}}$ (`subset_train_score_mapping`): Used to learn the mapping $M$ from the consensus score to the balancing parameter $\\theta$.\n",
    "\n",
    "This separation ensures that the mapping is learned on data not seen during the training of the candidate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, dataset_test = dataset_config.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we have to shuffle the indices because the validation dataset might\n",
    "# be sorted. However, because we pass a seed, the shuffling is fully\n",
    "# reproducible.\n",
    "indices = list(range(len(dataset_val)))\n",
    "random_generator = np.random.default_rng(seed)\n",
    "random_generator.shuffle(indices)\n",
    "subset_train_dfr = BiasSubset(dataset_val, indices=indices[:len(indices)//2])\n",
    "subset_train_score_mapping = BiasSubset(dataset_val, indices=indices[len(indices)//2:])\n",
    "subset_test = dataset_test\n",
    "\n",
    "print(\"#Samples Train DFR:\\t\\t\", len(subset_train_dfr))\n",
    "print(\"#Samples Train Score Mapping:\\t\", len(subset_train_score_mapping))\n",
    "print(\"#Samples Test:\\t\\t\\t\", len(subset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels.\n",
    "labels_train_dfr = np.array([subset_train_dfr.label(idx) for idx in range(len(subset_train_dfr))])\n",
    "labels_train_score_mapping = np.array([subset_train_score_mapping.label(idx) for idx in range(len(subset_train_score_mapping))])\n",
    "labels_test = np.array([subset_test.label(idx) for idx in range(len(subset_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader.\n",
    "dataloader_train_dfr = DataLoader(\n",
    "    subset_train_dfr,\n",
    "    batch_size=100,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_train_score_mapping = DataLoader(\n",
    "    subset_train_score_mapping,\n",
    "    batch_size=100,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    subset_test,\n",
    "    batch_size=100,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Model and Feature Extractor\n",
    "\n",
    "We load a standard ERM model pre-trained on the biased training set $\\mathcal{D}$.\n",
    "The feature extractor $e$ of this model will be fixed and used to generate feature representations for all subsequent steps.\n",
    "We consider multiple ERM models (trained with different seeds) to report mean and standard deviation over our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = dataset_config.ModelClass\n",
    "base_models = dataset_config.get_base_models()\n",
    "print(f\"Considering the following base models: {base_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Subgroup Identification\n",
    "\n",
    "As defined in the paper, we formalize bias as an imbalance between subgroups $\\mathcal{G}$, where each subgroup is a combination of a class label and a bias attribute. We identify these subgroups for each data split, as the subgroup labels are required for creating the re-weighted datasets $\\mathcal{D}_\\theta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubgroupIdentifier = dataset_config.SubgroupIdentifierClass\n",
    "print(f\"Using the following subgroup identification method: {SubgroupIdentifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_identifier = SubgroupIdentifier()\n",
    "group_ids_train_dfr, n_subgroups = subgroup_identifier.identify_subgroups(None, subset_train_dfr, device)\n",
    "group_ids_train_score_mapping, _ = subgroup_identifier.identify_subgroups(None, subset_train_score_mapping, device)\n",
    "group_ids_test, _ = subgroup_identifier.identify_subgroups(None, subset_test, device)\n",
    "group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test = [z.cpu().detach().numpy() for z in (group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test)]\n",
    "\n",
    "# Remove empty groups if desired (i.e., re-ordering such that empty groups are\n",
    "# listed last and then discarding them).\n",
    "if dataset_config.subgroup_postprocessor:\n",
    "    n_subgroups, group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test = dataset_config.subgroup_postprocessor(group_ids_train_dfr, group_ids_train_score_mapping, group_ids_test)\n",
    "\n",
    "# Are the subgroups balanced?\n",
    "print(\"Number of subgroups: \", n_subgroups)\n",
    "print(\"Subgroup balancing (train DFR):\", np.unique(group_ids_train_dfr, return_counts=True)[1])\n",
    "print(\"Subgroup balancing (train score mapping):\", np.unique(group_ids_train_score_mapping, return_counts=True)[1])\n",
    "print(\"Subgroup balancing (test subset):\", np.unique(group_ids_test, return_counts=True)[1])\n",
    "\n",
    "# Test for missing subgroups.\n",
    "# We expect that each subset contains all subgroups when performing the balanced\n",
    "# subsampling. Alternatively, one could adapt the BalancedSampler such that it\n",
    "# allows subgroups not to be present.\n",
    "assert len(set(group_ids_train_dfr)) == n_subgroups, \"No samples of the following subgroup(s) in subset_train_dfr: \" + str(set(range(n_subgroups)).difference(set(group_ids_train_dfr))) \n",
    "assert len(set(group_ids_train_score_mapping)) == n_subgroups, \"No samples of the following subgroup(s) in subset_train_score_mapping: \" + str(set(range(n_subgroups)).difference(set(group_ids_train_score_mapping)))\n",
    "assert len(set(group_ids_test)) == n_subgroups, \"No samples of the following subgroup(s) in subset_test: \" + str(set(range(n_subgroups)).difference(set(group_ids_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Simulating Test Distributions with Varying Bias\n",
    "\n",
    "To evaluate the robustness of our method, we simulate a range of test distributions with varying degrees of spurious correlation. We achieve this by re-sampling the validation and test sets according to the parameterized distribution $P_\\theta(Y, B) = \\theta \\, P_{\\mathrm{bal}}(Y,B) + (1-\\theta) \\, P_{\\mathrm{orig}}(Y,B)$, where $\\theta=0$ corresponds to the original biased distribution and $\\theta=1$ corresponds to a perfectly balanced one. The `BalancedSampler` generates subsets of indices that approximate these target distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset subsamples with the desired balancing factors.\n",
    "# Note that since we set the seed, all models are evaluated on the \n",
    "# same subsets (per balancing factor).\n",
    "balancing_mode = \"exact_under\"\n",
    "subset_indices_per_balancing_factor_train_dfr = [\n",
    "    list(\n",
    "        BalancedSampler(\n",
    "            subset_train_dfr,\n",
    "            mode=balancing_mode,\n",
    "            balancing_factor=balancing_factor,\n",
    "            labels=torch.tensor(group_ids_train_dfr),\n",
    "            seed=seed\n",
    "        )\n",
    "    )\n",
    "    for balancing_factor in balancing_factors\n",
    "]\n",
    "\n",
    "subset_indices_per_balancing_factor_train_score_mapping = [\n",
    "    list(\n",
    "        BalancedSampler(\n",
    "            subset_train_score_mapping,\n",
    "            mode=balancing_mode,\n",
    "            balancing_factor=balancing_factor,\n",
    "            labels=torch.tensor(group_ids_train_score_mapping),\n",
    "            seed=seed\n",
    "        )\n",
    "    )\n",
    "    for balancing_factor in balancing_factors\n",
    "]\n",
    "\n",
    "subset_indices_per_balancing_factor_test = [\n",
    "    list(\n",
    "        BalancedSampler(\n",
    "            subset_test,\n",
    "            mode=balancing_mode,\n",
    "            balancing_factor=balancing_factor,\n",
    "            labels=torch.tensor(group_ids_test),\n",
    "            seed=seed\n",
    "        )\n",
    "    )\n",
    "    for balancing_factor in balancing_factors\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below visualizes the effect of the balancing parameter $\\theta$ on the subgroup composition of the resulting test subsets. As $\\theta$ increases from 0 to 1, the proportions of all subgroups converge to be uniform, effectively removing the spurious correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5.0,3.2))\n",
    "\n",
    "for subgroup_id in np.unique(group_ids_test):\n",
    "    counts = [np.sum(group_ids_test[indices] == subgroup_id) / len(indices) for indices in subset_indices_per_balancing_factor_test]\n",
    "    plt.plot(balancing_factors, counts, label=\"Subgroup \" + str(subgroup_id + 1))\n",
    "\n",
    "    # Calculate intercept with the x-axis.\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(balancing_factors.reshape(-1, 1), counts)\n",
    "    slope = lm.coef_[0]\n",
    "    intercept = lm.intercept_\n",
    "    x_intercept = -intercept / slope\n",
    "    print(\"x-intercept for subgroup \" + str(subgroup_id + 1) + \": \" + str(x_intercept))\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(r\"Balancing Parameter $\\theta$ of the Test Distribution\")\n",
    "plt.ylabel(\"Percentage of Samples\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Candidate Model Construction (Step 1)\n",
    "\n",
    "This section implements the first step of AMSEL: constructing the family of candidate models $\\{m_\\theta = h_\\theta \\circ e | \\theta \\in \\Theta\\}$. We first evaluate the baseline ERM model and then proceed to train the candidate classifier heads.\n",
    "\n",
    "### 2.1. Baseline: ERM Model Performance\n",
    "\n",
    "As a primary baseline, we evaluate the performance of the original ERM model on the test set. This represents the standard approach without any debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = join(output_dir, dataset_name + \"_data_erm-pred-and-conf.pkl\")\n",
    "if isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as handle:\n",
    "        cache_data = pickle.load(handle)\n",
    "        erm_predictions = cache_data[\"predictions\"]\n",
    "        erm_confidence_scores = cache_data[\"confidence_scores\"]\n",
    "        print(\"Found ERM predictions/confidence scores created at \" + str(cache_data[\"creation_time\"]) + \", loading cached data instead of recalculation.\")\n",
    "\n",
    "else:\n",
    "    erm_predictions = []\n",
    "    erm_confidence_scores = []\n",
    "    for base_model in base_models:\n",
    "        print(\n",
    "            \"Generating predictions for ERM model: \"\n",
    "            + str(base_model)\n",
    "        )\n",
    "\n",
    "        # Load corresponding ERM model.\n",
    "        model = Model(root=root_dir, model=base_model, download=False).to(device)\n",
    "        model = model.eval()\n",
    "\n",
    "        # Conversion from model outputs to class label predictions.\n",
    "        # We could also define this function once for all base models.\n",
    "        if list(model.modules())[-1].out_features != 1:\n",
    "            model_outputs_to_preds_transform = Lambda(lambda x: torch.argmax(x, dim=1))\n",
    "        else:\n",
    "            model_outputs_to_preds_transform = Lambda(lambda x: (x >= 0.5).long())\n",
    "\n",
    "        # Extract predictions.\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            # Get confidence scores.\n",
    "            pred = [model(x[0].to(device)).cpu() for x in tqdm(dataloader_test)]\n",
    "\n",
    "            # Check whether SoftMax should be applied.\n",
    "            #   1. Multi-class output (sigmoid usually has a single output neuron)\n",
    "            #   2. Outputs are probabilities\n",
    "            #   3. Outputs are within [0,1]\n",
    "            # While this does not guarantee that SoftMax should be applied, it is\n",
    "            # as close as we can reasonably get without inspecting the model.\n",
    "            conf = torch.cat(pred)\n",
    "            if len(conf.shape) == 2 and conf.shape[1] >= 2 and not (torch.all(torch.isclose(conf.sum(dim=-1), torch.tensor(1.0, device=conf.device, dtype=conf.dtype))) and conf.min() >= 0 and conf.max() <= 1):\n",
    "                warnings.warn(\"It seems that SoftMax has not been applied to the network output, applying it now. Please check that this is appropriate for your model!\")\n",
    "                conf = softmax(conf, dim=-1)\n",
    "            elif len(conf.shape) == 1 and conf.min() >= 0 and conf.max() <= 1:\n",
    "                warnings.warn(\"Converting true class probabilities to per-class probabilities.\")\n",
    "                conf = torch.stack([1 - conf, conf], dim=1)\n",
    "            erm_confidence_scores.append(conf)\n",
    "\n",
    "            # Convert into predictions.\n",
    "            pred = torch.cat([model_outputs_to_preds_transform(p) for p in tqdm(pred)])\n",
    "            erm_predictions.append(pred)\n",
    "    erm_predictions = np.array(erm_predictions)\n",
    "    erm_confidence_scores = np.array(erm_confidence_scores)\n",
    "\n",
    "    # Save created predictions/conf. scores for future repeated experiments.\n",
    "    with open(cache_file, \"wb\") as handle:\n",
    "        print(\"Saving ERM predictions/confidence scores scores at:\", cache_file)\n",
    "        pickle.dump(\n",
    "            {\"predictions\": erm_predictions, \"confidence_scores\": erm_confidence_scores, \"creation_time\": datetime.now()},\n",
    "            handle,\n",
    "            protocol=pickle.HIGHEST_PROTOCOL\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy and Worst-Subgroup Accuracy.\n",
    "erm_accuracies = np.array([accuracy_score(labels_test, p) for p in erm_predictions])\n",
    "erm_wsg_accuracies = np.array(\n",
    "    [worst_subgroup_accuracy(labels_test, p, group_ids_test) for p in erm_predictions]\n",
    ")\n",
    "erm_per_subgroup_accuracies = np.array(\n",
    "    [subgroup_accuracy(labels_test, p, group_ids_test, aggregation=\"none\") for p in erm_predictions]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracies: \" + str(erm_accuracies))\n",
    "print(\"Worst-Group Accuracies: \" + str(erm_wsg_accuracies))\n",
    "print(\"Per-Subgroup Accuracies: \" + str([f\"{np.mean(a):.2%}±{np.std(a):.2%}\" for a in erm_per_subgroup_accuracies.T]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Training Candidate Classifiers\n",
    "\n",
    "Here, we construct the family of candidate classifier heads $\\{h_\\theta\\}$. For each ERM backbone, we first extract features from all relevant data splits. Then, for each balancing parameter $\\theta$, we train a logistic regression classifier $h_\\theta$ on features extracted from a corresponding subset of $\\mathcal{D}_{\\mathrm{candidate}}$. This process, which implements Eq. (2) from the paper, results in a set of candidate models, each specialized for a different level of spurious correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = join(output_dir, dataset_name + \"_data_tasks.pkl\")\n",
    "if isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as handle:\n",
    "        cache_data = pickle.load(handle)\n",
    "        tasks = cache_data[\"tasks\"]\n",
    "        print(\"Found pre-generated features and classifier heads created at \" + str(cache_data[\"creation_time\"]) + \", loading cached data instead of recalculation.\")\n",
    "\n",
    "else:\n",
    "    # Train models and extract outputs.\n",
    "    tasks = {}\n",
    "    for base_model_id, base_model in enumerate(base_models):   \n",
    "        print(\n",
    "            \"Generating reweighted DFR models and extracting model outputs for DFR models with feature backbone: \"\n",
    "            + str(base_model)\n",
    "        )\n",
    "\n",
    "        # Reproducibility.\n",
    "        # We set the seed explicitly so that this cell is re-producible even when\n",
    "        # reexecuted (standalone reproducibility).\n",
    "        np.random.seed(seed + base_model_id)\n",
    "\n",
    "        # Load corresponding ERM model.\n",
    "        model = Model(root=root_dir, model=base_model, download=False)\n",
    "\n",
    "        # Extract training and test features.\n",
    "        # The resulting feature vectors have shape [n_samples, n_feature_dims].\n",
    "        with torch.no_grad():\n",
    "            # Extract features & labels from subset_train_dfr.\n",
    "            feature_extractor = model.to(device).get_feature_extractor().eval()\n",
    "            embeddings, labels = extract_features(\n",
    "                feature_extractor=feature_extractor,\n",
    "                dataloader=dataloader_train_dfr,\n",
    "                extract_labels=True,\n",
    "                device=device,\n",
    "            )\n",
    "            X_train_dfr, y_train_dfr = [x.detach().cpu().numpy() for x in (embeddings, labels)]\n",
    "\n",
    "            # Extract features & labels from subset_train_score_mapping.\n",
    "            embeddings, labels = extract_features(\n",
    "                feature_extractor=feature_extractor,\n",
    "                dataloader=dataloader_train_score_mapping,\n",
    "                extract_labels=True,\n",
    "                device=device,\n",
    "            )\n",
    "            X_train_score_mapping, y_train_score_mapping = [x.detach().cpu().numpy() for x in (embeddings, labels)]\n",
    "\n",
    "            # Extract features & labels from subset_test.\n",
    "            embeddings, labels = extract_features(\n",
    "                feature_extractor=feature_extractor,\n",
    "                dataloader=dataloader_test,\n",
    "                extract_labels=True,\n",
    "                device=device,\n",
    "            )\n",
    "            X_test, y_test = [x.detach().cpu().numpy() for x in (embeddings, labels)]\n",
    "        feature_extractor = feature_extractor.to(\"cpu\")\n",
    "        model = model.to(\"cpu\")\n",
    "\n",
    "        # torch does not realize that this would be a good point to perform garbage\n",
    "        # collection on the feature extraction step. Thus, we have to actively call\n",
    "        # it.\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Define and train DFR models.\n",
    "        dfr_models = []\n",
    "        for balancing_factor in tqdm(\n",
    "            balancing_factors, desc=\"Train DFR models\"\n",
    "        ):\n",
    "            # Select appropriate solver for the logistic regression.\n",
    "            if len(subset_test.class_labels) == 2:\n",
    "                clf = CustomLogisticRegression(\n",
    "                    num_retrains=20,\n",
    "                    balancing_factor=balancing_factor,\n",
    "                    C=regularization_parameter_c,\n",
    "                    penalty=\"l1\",\n",
    "                    solver=\"liblinear\",\n",
    "                    random_state=seed,\n",
    "                    class_weight=\"balanced\",\n",
    "                    # verbose=1,\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\"Multiclass problem, selecting different solver and penalty because this is not supported by liblinear!\")\n",
    "                clf = CustomLogisticRegression(\n",
    "                    num_retrains=20,\n",
    "                    balancing_factor=balancing_factor,\n",
    "                    C=regularization_parameter_c,\n",
    "                    solver=\"lbfgs\",\n",
    "                    n_jobs=-1,\n",
    "                    # penalty=\"l1\",\n",
    "                    random_state=seed,\n",
    "                    class_weight=\"balanced\",\n",
    "                    # verbose=1,\n",
    "                )      \n",
    "\n",
    "            # Retrain classifier head (with appropriate balancing).\n",
    "            dfr = DeepFeatureReweighting(classifier=clf)\n",
    "            dfr_model = dfr._remove_shortcut(\n",
    "                model=model,\n",
    "                feature_extractor=feature_extractor,\n",
    "                X=X_train_dfr,\n",
    "                y=y_train_dfr,\n",
    "                group_ids=group_ids_train_dfr,\n",
    "                device=device,\n",
    "            )\n",
    "            dfr_models.append(dfr_model)\n",
    "\n",
    "        # Extract model predictions on the training dataset for the score mapping\n",
    "        # and on the test dataset.\n",
    "        outputs_test = []\n",
    "        for model in tqdm(dfr_models, desc=\"Predict probabilities for: subset_test\"):\n",
    "            x = np.copy(X_test)\n",
    "\n",
    "            # Preprocessing and classification.\n",
    "            if model.preprocess:\n",
    "                x = model.scaler.transform(x)\n",
    "            x = model.classifier.predict_proba(x)\n",
    "            \n",
    "            outputs_test.append(x)\n",
    "        outputs_test = np.array(outputs_test)\n",
    "\n",
    "        outputs_train_score_mapping = []\n",
    "        for model in tqdm(dfr_models, desc=\"Predict probabilities for: subset_train_score_mapping\"):\n",
    "            x = np.copy(X_train_score_mapping)\n",
    "\n",
    "            # Preprocessing and classification.\n",
    "            if model.preprocess:\n",
    "                x = model.scaler.transform(x)\n",
    "            x = model.classifier.predict_proba(x)\n",
    "            outputs_train_score_mapping.append(x)\n",
    "        outputs_train_score_mapping = np.array(outputs_train_score_mapping)\n",
    "\n",
    "        tasks[base_model] = {\n",
    "            \"X_train_dfr\" : X_train_dfr,\n",
    "            \"y_train_dfr\" : y_train_dfr,\n",
    "            \"X_train_score_mapping\" : X_train_score_mapping,\n",
    "            \"y_train_score_mapping\" : y_train_score_mapping,\n",
    "            \"X_test\" : X_test,\n",
    "            \"y_test\" : y_test,\n",
    "            \"outputs_train_score_mapping\" : outputs_train_score_mapping,\n",
    "            \"outputs_test\" : outputs_test,\n",
    "            \"dfr_models\" : dfr_models,\n",
    "        }\n",
    "\n",
    "    print(\"Extracted features of shape \" + str(X_test.shape) + \" for testing.\")\n",
    "\n",
    "    # Save created PD scores for future repeated experiments.\n",
    "    with open(cache_file, \"wb\") as handle:\n",
    "        print(\"Saving features and classifier heads at:\", cache_file)\n",
    "        pickle.dump(\n",
    "            {\"tasks\": tasks, \"creation_time\": datetime.now()},\n",
    "            handle,\n",
    "            protocol=pickle.HIGHEST_PROTOCOL\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test dataset.\n",
    "dfr_predictions = []\n",
    "for i, base_model in enumerate(base_models):\n",
    "    # Load predicted probabilities.\n",
    "    outputs_test = tasks[base_model][\"outputs_test\"]\n",
    "\n",
    "    # Generate predictions for all candidate models/all balancing factors.\n",
    "    # For this, we have to convert the predicted probabilities into class\n",
    "    # predictions.\n",
    "    assert len(outputs_test.shape) == 3, \"Expected shape of predicted probabilities is [n_balancing_factors, n_samples, n_classes]!\"\n",
    "    p = np.argmax(outputs_test, axis=2) # Shape: [n_balancing_factors, n_samples, n_classes] -> [n_balancing_factors, n_samples].\n",
    "    p = p.transpose() # Shape: [n_balancing_factors, n_samples] -> [n_samples, n_balancing_factors].\n",
    "    \n",
    "    dfr_predictions.append(p)\n",
    "dfr_predictions = np.array(\n",
    "    dfr_predictions\n",
    ")  # Shape: [n_base_models, n_samples, n_balancing_factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy.\n",
    "dfr_accuracies = np.array(\n",
    "    [\n",
    "        [accuracy_score(labels_test, p) for p in predictions.T]\n",
    "        for predictions in dfr_predictions\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Worst Subgroup Accuracy.\n",
    "dfr_wsg_accuracies = np.array(\n",
    "    [\n",
    "        [worst_subgroup_accuracy(labels_test, p, group_ids_test) for p in predictions_per_balancing_factor.T]\n",
    "        for predictions_per_balancing_factor in dfr_predictions\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaptive Model Selection (Steps 2 & 3)\n",
    "\n",
    "This section implements the core adaptive mechanism of AMSEL. We first define the score function used to characterize the test distribution, then learn the mapping from this score to the balancing parameter, and finally perform adaptive inference.\n",
    "\n",
    "### 3.1. Score Function for Bias Estimation\n",
    "\n",
    "To estimate the level of spurious correlation in a given dataset, we use a score function $s(\\cdot)$. As described in the paper, our implementation uses a **consensus score**, $s_{\\mathrm{consensus}}$, which measures the prediction agreement between the two most extreme candidate models: $m_0$ (trained on the original biased data, $\\theta=0$) and $m_1$ (trained on a fully balanced dataset, $\\theta=1$). The intuition is that these models will agree on bias-aligned samples but disagree on bias-conflicting ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(func):\n",
    "    # Inputs of the score functions are predicted probabilities with shape\n",
    "    # [n_balancing_factors, n_samples, n_classes]. The predicted probabilities\n",
    "    # should be limited to the samples of interest.\n",
    "    # Note that most score functions should be able to calculate their scores\n",
    "    # based solely on the predicted probabilities. However, we also allow to\n",
    "    # re-calculate the scores from scratch by passing the ID of the base model\n",
    "    # and the considered indices of the samples in question.\n",
    "    def wrapper(predicted_probabilties: np.ndarray, base_model_id: int, indices: np.ndarray, subset_name: str):\n",
    "        assert len(predicted_probabilties.shape) == 3, \"Error: Expected shape of predicted probabilities to be [n_balancing_factors, n_samples, n_classes]!\"\n",
    "        assert predicted_probabilties.shape[2] >= 2, \"Error: Expected number of classes to be >= 2!\"\n",
    "\n",
    "        predicted_probabilties_biased = predicted_probabilties[0,:,:] # Shape: [n_samples, n_classes].\n",
    "        predicted_probabilties_unbiased = predicted_probabilties[unbiased_model_idx,:,:] # Shape: [n_samples, n_classes].\n",
    "\n",
    "        return func(predicted_probabilties_biased, predicted_probabilties_unbiased)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@score_func\n",
    "def consensus_level(predicted_probabilties_biased: np.ndarray, predicted_probabilties_unbiased: np.ndarray):\n",
    "    # Convert the predicted probabilities into class predictions.\n",
    "    assert len(predicted_probabilties_biased.shape) == 2, \"Expected shape of predicted probabilities is [n_samples, n_classes]!\"\n",
    "    assert len(predicted_probabilties_unbiased.shape) == 2, \"Expected shape of predicted probabilities is [n_samples, n_classes]!\"\n",
    "\n",
    "    predictions_biased = np.argmax(predicted_probabilties_biased, axis=1) # Shape: [n_samples, n_classes] -> [n_samples].\n",
    "    predictions_unbiased = np.argmax(predicted_probabilties_unbiased, axis=1) # Shape: [n_samples, n_classes] -> [n_samples].\n",
    "\n",
    "    # Do the two models agree?\n",
    "    return np.mean(predictions_biased == predictions_unbiased)\n",
    "\n",
    "score_functions = {\n",
    "    \"Consensus\": consensus_level,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below validates our choice of score function. It shows a strong, near-linear relationship between the true balancing parameter $\\theta$ of the test distribution and the observed consensus score. This strong correlation makes it possible to learn an effective mapping from the score to the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tick_descriptions(\n",
    "    fig,\n",
    "    ax,\n",
    "    x,\n",
    "    descs=[\"(Orig. Dist.)\", \"(Unbiased Dist.)\"],\n",
    "    vertical_offset_factor=1.2,\n",
    "    horizontal_offset=34,\n",
    "):\n",
    "    # x: x positions (what you would pass to plt.plot(x, y)).\n",
    "    \n",
    "    # Add custom tick description.\n",
    "    # 1) draw so that the tick‐layout is finalised\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # 2) grab the default tick‐label font size (in points)\n",
    "    sample_lbl = ax.get_xticklabels()[0]\n",
    "    fs_pts     = sample_lbl.get_fontsize()  \n",
    "\n",
    "    # 3) settings\n",
    "    # vertical offset = one line‐height (here 1.2× font‐size for a little extra breathing)\n",
    "    vertical_offset = -vertical_offset_factor * fs_pts  \n",
    "    # descs = [\"(Orig. Dist.)\", \"(Unbiased Dist.)\"]\n",
    "    ends = [min(x), max(x)] # [0.0, max(x)]\n",
    "\n",
    "    # 4) annotate each “description” one line below the tick, shifted ±50 points horizontally\n",
    "    for xval, desc in zip(ends, descs):\n",
    "        ha = \"left\" if xval == ends[0] else \"right\"\n",
    "        dx = -horizontal_offset if ha == \"left\" else +horizontal_offset\n",
    "\n",
    "        ax.annotate(\n",
    "            desc,\n",
    "            xy=(xval, 0),                         # anchor at the tick line\n",
    "            xycoords=ax.get_xaxis_transform(),    # x in data‐units, y=0 at the axis\n",
    "            xytext=(dx, vertical_offset),         # (±50 pts, –1 line) in points\n",
    "            textcoords=\"offset points\",\n",
    "            ha=ha, va=\"top\",\n",
    "            fontsize=fs_pts,                      # match the tick‐label size\n",
    "            clip_on=False                         # allow drawing outside the axis\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure score function values for each base model.\n",
    "score_function = consensus_level\n",
    "score_values = [] # Later shape: [n_base_models, n_balancing_factors]\n",
    "for base_model_id, base_model in enumerate(base_models):\n",
    "    # Get predicted probabilities for the test set.\n",
    "    outputs_test = tasks[base_model][\"outputs_test\"] # Shape: [n_balancing_factors, n_samples, n_classes].\n",
    "\n",
    "    # Calculate score function.\n",
    "    scores_test = np.array([score_function(outputs_test[:,indices,:], base_model_id=base_model_id, indices=indices, subset_name=\"test\") for indices in subset_indices_per_balancing_factor_test])\n",
    "\n",
    "    score_values.append(scores_test)\n",
    "score_values = np.array(score_values)\n",
    "\n",
    "# Calculate Pearson correlation coefficients.\n",
    "correlation_coefs = [np.corrcoef(balancing_factors, vals)[1,0] for vals in score_values]\n",
    "\n",
    "# Plot.\n",
    "mean = np.mean(score_values, axis=0)\n",
    "std = np.std(score_values, axis=0)\n",
    "\n",
    "plt.figure(figsize=(7.0,3.0))\n",
    "plt.plot(balancing_factors, mean, color=\"C0\")\n",
    "plt.fill_between(balancing_factors, mean+std, mean-std, facecolor=\"C0\", alpha=0.3)\n",
    "plt.xlabel(r\"Balancing Parameter $\\theta$ of the Test Distribution\", labelpad=20) # Because we add the second row of the xticks manually\n",
    "plt.ylabel(\"Consensus Score\")\n",
    "\n",
    "plt.figtext(\n",
    "    x=0.733,\n",
    "    y=0.84,\n",
    "    s=(f\"$r={np.mean(correlation_coefs): .2f}\\\\pm{np.std(correlation_coefs): .2f}$\"),\n",
    "    horizontalalignment='left',\n",
    ")\n",
    "\n",
    "plt.figtext(\n",
    "    x=0.03,\n",
    "    y=-0.1,\n",
    "    s=(\"Experiment: \" + dataset_name + f\" - Consensus Level\\nPearson Correlation: {np.mean(correlation_coefs): .4f} \" + r\"$\\pm$\" + f\" {np.std(correlation_coefs): .4f}\"),\n",
    "    horizontalalignment='left',\n",
    "    style=\"italic\"\n",
    ")\n",
    "\n",
    "# Add custom tick description.\n",
    "add_tick_descriptions(\n",
    "    fig,\n",
    "    plt.gca(),\n",
    "    balancing_factors,\n",
    "    descs=[\"(Orig. Dist.)\", \"(Unbiased Dist.)\"],\n",
    "    vertical_offset_factor=1.5,\n",
    "    horizontal_offset=28,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(output_dir, dataset_name + \" - dependency consensus level and balancing factor.pdf\"), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Full Pipeline: Learning the Mapping and Adaptive Inference\n",
    "\n",
    "This section combines the final two steps of the AMSEL pipeline.\n",
    "\n",
    "**Step 2: Learning the Mapping $M$**\n",
    "We use the `subset_train_score_mapping` ($\\mathcal{D}_{\\mathrm{mapping}}$) to learn the mapping $M: s(\\mathcal{X}^n) \\to \\Theta$. We calculate the consensus score for subsets of $\\mathcal{D}_{\\mathrm{mapping}}$ with varying balancing parameters and train a linear regression model to predict $\\theta$ from the score.\n",
    "\n",
    "**Step 3: Adaptive Inference**\n",
    "During the evaluation phase, for each simulated test distribution, we:\n",
    "1.  Calculate its consensus score.\n",
    "2.  Use the learned regressor $M$ to estimate the test balancing parameter, $\\theta_{\\mathrm{test}}$.\n",
    "3.  Select the candidate model $m_{\\theta_{\\mathrm{test}}}$ whose training parameter is closest to the estimated $\\theta_{\\mathrm{test}}$.\n",
    "4.  Evaluate the performance of the selected model on the given test distribution.\n",
    "\n",
    "The `amsel` function below encapsulates this entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amsel(\n",
    "    tasks=tasks,\n",
    "    balancing_factors=balancing_factors,\n",
    "    group_ids_train_score_mapping=group_ids_train_score_mapping,\n",
    "    group_ids_test=group_ids_test,\n",
    "    subset_train_score_mapping=subset_train_score_mapping,\n",
    "    subset_test=subset_test,\n",
    "    erm_predictions=erm_predictions,\n",
    "    dfr_predictions=dfr_predictions,\n",
    "    amsel_mode=\"default\",\n",
    "    model_selection_strategy=\"naive_model_selection\",\n",
    "    score_function=consensus_level,\n",
    "    sample_size=None,\n",
    "    seed=seed,\n",
    "):\n",
    "    base_models = list(tasks.keys())\n",
    "    results = {base_model: {} for base_model in base_models}\n",
    "\n",
    "    if amsel_mode == \"default\": # Interpolated balancing + retraining of all classifier heads\n",
    "        model_prefix = \"\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode for AMSEL specified. Please select one of 'default'. Given mode was: \" + str(amsel_mode))\n",
    "    \n",
    "    for base_model_id, base_model in enumerate(base_models):\n",
    "        # Extract model outputs.\n",
    "        outputs_train_score_mapping, outputs_test, y_train_score_mapping, y_test = [\n",
    "            tasks[base_model][z]\n",
    "            for z in [\n",
    "                \"outputs_\" + model_prefix + \"train_score_mapping\",\n",
    "                \"outputs_\" + model_prefix + \"test\",\n",
    "                \"y_train_score_mapping\",\n",
    "                \"y_test\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        # Training Phase.\n",
    "        # 1) Generate subsets with different balancing factors.\n",
    "        # 2) Identify which classifier performs best for each balancing factor. For\n",
    "        #    this, we consider overall accuracy.\n",
    "        # 3) Learn mapping from the score function to balancing factor\n",
    "        #    \\alpha of the dataset.\n",
    "\n",
    "        # 1. Generate subsets.\n",
    "        # Generate dataset subsamples with the desired balancing factors.\n",
    "        # Note that since we set the seed, all models are evaluated on the \n",
    "        # same subsets (per balancing factor).\n",
    "        subset_indices_per_balancing_factor_train_score_mapping = [\n",
    "            list(\n",
    "                BalancedSampler(\n",
    "                    subset_train_score_mapping,\n",
    "                    mode=\"exact_under\",\n",
    "                    balancing_factor=balancing_factor,\n",
    "                    labels=torch.tensor(group_ids_train_score_mapping),\n",
    "                    seed=seed\n",
    "                )\n",
    "            )\n",
    "            for balancing_factor in balancing_factors\n",
    "        ]\n",
    "\n",
    "        # 2. Identify optimal model per balancing factor.\n",
    "        # Generate predictions for the training split (subset_train_score_mapping).\n",
    "        assert len(outputs_train_score_mapping.shape) == 3, \"Expected shape of predicted probabilities is [n_balancing_factors, n_samples, n_classes]!\"\n",
    "        predictions_train_score_mapping = np.argmax(outputs_train_score_mapping, axis=2) # Shape: [n_balancing_factors, n_samples, n_classes] -> [n_balancing_factors, n_samples].\n",
    "        predictions_train_score_mapping = predictions_train_score_mapping.transpose() # Shape: [n_balancing_factors, n_samples] -> [n_samples, n_balancing_factors].\n",
    "\n",
    "        # For each balancing factor, get the best-performing model.\n",
    "        optimal_dfr_model_indices = np.array([np.argmax([accuracy_score(y_train_score_mapping[indices], p[indices]) for p in predictions_train_score_mapping.T]) for indices in subset_indices_per_balancing_factor_train_score_mapping])\n",
    "\n",
    "        # 3. Learn mapping for balancing factor.\n",
    "        # Note that technically speaking, we don't predict the balancing factor but\n",
    "        # rather the index of the balancing factor, assuming ascending and\n",
    "        # equidistant balancing factors.\n",
    "        # Here, we first have to measure the consensus level (or whatever metric\n",
    "        # we are currently considering) and then train a linear regression to\n",
    "        # map consensus level -> balancing parameter.\n",
    "        scores_train_score_mapping = np.array([score_function(outputs_train_score_mapping[:,indices,:], base_model_id=base_model_id, indices=indices, subset_name=\"train_score_mapping\") for indices in subset_indices_per_balancing_factor_train_score_mapping])\n",
    "        x = scores_train_score_mapping.reshape(-1, 1)\n",
    "        y = balancing_factors # np.arange(len(scores_train_score_mapping))\n",
    "        # y = y / np.max(y) # Map to [0,1] -> Just predict actual balancing parameter.\n",
    "        if score_mapping == \"linear_regression\":\n",
    "            reg = LinearRegression()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid estimator for score mapping specified!\")\n",
    "        reg.fit(x, y)\n",
    "\n",
    "        # Evaluation Phase.\n",
    "        # 1) Generate subsets with desired balancing factors to evaluate.\n",
    "        # 2) Measure consensus level.\n",
    "        # 3) Estimate the balancing factor of the given dataset based on the\n",
    "        #    consensus level.\n",
    "        # 4) Choose the appropriate model based on the identified balancing factor\n",
    "        #    and evaluate with that model.\n",
    "        # 5) Compare to default ERM and DFR models.\n",
    "\n",
    "        # 1. Generate subsets.\n",
    "        # Generate dataset subsamples with the desired balancing factors.\n",
    "        # Note that since we set the seed, all models are evaluated on the \n",
    "        # same subsets (per balancing factor).\n",
    "        subset_indices_per_balancing_factor_test = [\n",
    "            list(\n",
    "                BalancedSampler(\n",
    "                    subset_test,\n",
    "                    mode=\"exact_under\",\n",
    "                    balancing_factor=balancing_factor,\n",
    "                    labels=torch.tensor(group_ids_test),\n",
    "                    seed=seed\n",
    "                )\n",
    "            )\n",
    "            for balancing_factor in balancing_factors\n",
    "        ]\n",
    "\n",
    "        # Generate predictions for the test split (subset_test).\n",
    "        assert len(outputs_test.shape) == 3, \"Expected shape of predicted probabilities is [n_balancing_factors, n_samples, n_classes]!\"\n",
    "        predictions_test = np.argmax(outputs_test, axis=2) # Shape: [n_balancing_factors, n_samples, n_classes] -> [n_balancing_factors, n_samples].\n",
    "        predictions_test = predictions_test.transpose() # Shape: [n_balancing_factors, n_samples] -> [n_samples, n_balancing_factors].\n",
    "\n",
    "        # 2. Measure score function.\n",
    "        # Here, we estimate the scores on the whole test dataset. To simulate an\n",
    "        # online-setting, one could also estimate the consensus level only on a\n",
    "        # subset of all test samples (and use the selected model to predict for\n",
    "        # all test samples).\n",
    "        if sample_size is None:\n",
    "            scores_test = np.array([score_function(outputs_test[:,indices,:], base_model_id=base_model_id, indices=indices, subset_name=\"test\") for indices in subset_indices_per_balancing_factor_test])\n",
    "        else:\n",
    "            # Note that we have to use the same indices for slicing outputs_test\n",
    "            # that we also pass to 'indices'. Thus, we have to re-create the\n",
    "            # random generator with the same seed.\n",
    "            scores_test = np.array([score_function(\n",
    "                outputs_test[:, np.random.default_rng(seed).choice(indices, [sample_size], replace=False), :], # Only consider subset with sample_size samples.\n",
    "                base_model_id=base_model_id,\n",
    "                indices=np.random.default_rng(seed).choice(indices, [sample_size], replace=False),\n",
    "                subset_name=\"test\"\n",
    "            ) for indices in subset_indices_per_balancing_factor_test])\n",
    "            \n",
    "        # 3. Estimate balancing factor.\n",
    "        # Similarly to above, we technically don't estimate the balancing factor\n",
    "        # itself but rather its index in balancing_factors.\n",
    "        # Note that we have to to ensure that all indices are within bounds\n",
    "        # (linear regression might predict something else). For simplicity, we\n",
    "        # simply clip the values to the desired interval instead of ensuring\n",
    "        # this property in the classification procedure.\n",
    "        x_test = scores_test.reshape(-1, 1)\n",
    "        # Note:\n",
    "        # The indices of the balancing factor are enumerated starting from zero:\n",
    "        # [0, ..., n-1]. Thus, for scaling we have to scale with *(n-1) instead\n",
    "        # of *n. \n",
    "        balancing_factor = reg.predict(x_test)\n",
    "        balancing_factor_idx = np.round(balancing_factor_to_index(balancing_factor, clip=True)).astype(int)\n",
    "        # balancing_factor_idx = np.round(reg.predict(x_test) / np.max(balancing_factors) * (len(scores_train_score_mapping) - 1)).astype(int)\n",
    "        # balancing_factor_idx = np.clip(balancing_factor_idx, a_min=0, a_max=len(balancing_factors) - 1)\n",
    "\n",
    "        # 4. Select the optimal model and evaluate.\n",
    "        # Depending on the model selection strategy, we either select the model\n",
    "        # trained on the simulated distribution with the same balancing factor\n",
    "        # or the model that performed best on the simulated distribution with\n",
    "        # this balancing factor.\n",
    "        if model_selection_strategy == \"optimal_model_selection\":\n",
    "            dfr_model_indices = optimal_dfr_model_indices[balancing_factor_idx]\n",
    "        elif model_selection_strategy == \"naive_model_selection\":\n",
    "            dfr_model_indices = balancing_factor_idx\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model selection strategy specified: \" + str(model_selection_strategy))\n",
    "        results[base_model][\"amsel_\" + amsel_mode + \"_\"  + model_selection_strategy] = np.array([accuracy_score(y_true=y_test[indices], y_pred=predictions_test[indices][:,dfr_model_idx]) for indices, dfr_model_idx in zip(subset_indices_per_balancing_factor_test, dfr_model_indices)])\n",
    "        results[base_model][\"amsel_\" + amsel_mode + \"_\"  + model_selection_strategy + \"_balanced\"] = np.array([balanced_accuracy_score(y_true=y_test[indices], y_pred=predictions_test[indices][:,dfr_model_idx]) for indices, dfr_model_idx in zip(subset_indices_per_balancing_factor_test, dfr_model_indices)])\n",
    "        results[base_model][\"amsel_\" + amsel_mode + \"_\"  + model_selection_strategy + \"_wsg\"] = np.array([worst_subgroup_accuracy(y_true=y_test[indices], y_pred=predictions_test[indices][:,dfr_model_idx], group_ids=group_ids_test[indices]) for indices, dfr_model_idx in zip(subset_indices_per_balancing_factor_test, dfr_model_indices)])\n",
    "        \n",
    "\n",
    "        # 5. Evaluate ERM and DFR model on the same subsplits.\n",
    "        results[base_model][\"erm_model\"] = np.array([accuracy_score(y_true=y_test[indices], y_pred=erm_predictions[base_model_id,indices]) for indices in subset_indices_per_balancing_factor_test])\n",
    "        results[base_model][\"dfr_model\"] = np.array([accuracy_score(y_true=y_test[indices], y_pred=dfr_predictions[base_model_id,indices,unbiased_model_idx]) for indices in subset_indices_per_balancing_factor_test])\n",
    "        results[base_model][\"erm_model_balanced\"] = np.array([balanced_accuracy_score(y_true=y_test[indices], y_pred=erm_predictions[base_model_id,indices]) for indices in subset_indices_per_balancing_factor_test])\n",
    "        results[base_model][\"dfr_model_balanced\"] = np.array([balanced_accuracy_score(y_true=y_test[indices], y_pred=dfr_predictions[base_model_id,indices,unbiased_model_idx]) for indices in subset_indices_per_balancing_factor_test])\n",
    "        results[base_model][\"erm_model_wsg\"] = np.array([worst_subgroup_accuracy(y_true=y_test[indices], y_pred=erm_predictions[base_model_id,indices], group_ids=group_ids_test[indices]) for indices in subset_indices_per_balancing_factor_test])\n",
    "        results[base_model][\"dfr_model_wsg\"] = np.array([worst_subgroup_accuracy(y_true=y_test[indices], y_pred=dfr_predictions[base_model_id,indices,unbiased_model_idx], group_ids=group_ids_test[indices]) for indices in subset_indices_per_balancing_factor_test])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_strategy = \"naive_model_selection\" # One of \"naive_model_selection\", \"optimal_model_selection\".\n",
    "score_function = score_functions[score_function_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {base_model: {} for base_model in base_models}\n",
    "for amsel_mode in [\"default\"]:\n",
    "    r = amsel(\n",
    "        amsel_mode=amsel_mode,\n",
    "        model_selection_strategy=model_selection_strategy,\n",
    "        score_function=score_function,\n",
    "        sample_size=None,\n",
    "    )\n",
    "    results = {k: results[k] | r[k] for k in results.keys()} # Merge results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and Analysis\n",
    "\n",
    "In this final section, we evaluate AMSEL and compare its performance against several baselines:\n",
    "-   **ERM**: The standard model trained with Empirical Risk Minimization.\n",
    "-   **DFR**: Deep Feature Reweighting, which is equivalent to our candidate model $m_1$ (i.e., $\\theta=1$), a fixed debiasing strategy.\n",
    "-   **EvA**: Erasing with Activations, another post-hoc debiasing method.\n",
    "\n",
    "### 4.1. Baseline: Erasing with Activations (EvA)\n",
    "\n",
    "We first implement and evaluate the EvA baseline. EvA identifies and removes spurious feature dimensions by comparing feature distributions from biased and unbiased data.\n",
    "\n",
    "We train the EvA classifiers on `subset_train_dfr` and select the best-performing classifier (i.e., hyperparameter optimization for the number of neurons to deactivate per class) on `subset_train_bootstrapping`. Then, we evaluate on `subset_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_visualization = False\n",
    "eva_predictions = []\n",
    "for base_model_id, base_model in enumerate(base_models):\n",
    "    # 1. Preparation.\n",
    "    # Outputs and labels.\n",
    "    X_train_dfr, X_train_score_mapping, X_test, y_train_dfr, y_train_score_mapping, y_test = [\n",
    "            tasks[base_model][z]\n",
    "            for z in [\n",
    "                \"X_train_dfr\", # Shape: [n_samples, n_features].\n",
    "                \"X_train_score_mapping\", # Shape: [n_samples, n_features].\n",
    "                \"X_test\", # Shape: [n_samples, n_features].\n",
    "                \"y_train_dfr\",\n",
    "                \"y_train_score_mapping\",\n",
    "                \"y_test\",\n",
    "            ]\n",
    "        ]\n",
    "    indices_balanced_train_dfr = subset_indices_per_balancing_factor_train_dfr[unbiased_model_idx]\n",
    "    indices_balanced_train_score_mapping = subset_indices_per_balancing_factor_train_score_mapping[unbiased_model_idx]\n",
    "\n",
    "    # Datasets.\n",
    "    # For EvA, we need a biased and an unbiased dataset. We consider\n",
    "    # subset_train_dfr to be our biased dataset and create a balanced version of\n",
    "    # subset_train_bootstrapping as our unbiased dataset.\n",
    "    X_biased = X_train_dfr\n",
    "    y_biased = y_train_dfr\n",
    "    group_ids_biased = group_ids_train_dfr\n",
    "    X_unbiased = X_train_score_mapping[indices_balanced_train_score_mapping]\n",
    "    y_unbiased = y_train_score_mapping[indices_balanced_train_score_mapping]\n",
    "    group_ids_unbiased = group_ids_train_score_mapping[indices_balanced_train_score_mapping]\n",
    "\n",
    "    # 2. Identify Spurious Features.\n",
    "    # Since we have access to an unbalanced variant of the dataset, we can\n",
    "    # identify the spurious feature dimensions via the consistency measure\n",
    "    # proposed by He et al. Note that the consistency is measured per feature\n",
    "    # dimension i and per class K.\n",
    "    #   c_ik = -d(\\phi^(i,k)_{train}, \\phi^(i,k)_{unbiased})\n",
    "    n_classes = len(subset_train_dfr.class_labels)\n",
    "    n_features = X_biased.shape[1]\n",
    "    consistency = np.array([[- wasserstein_distance(X_biased[(y_biased == k)][:,i], X_unbiased[y_unbiased == k][:,i]) for k in range(n_classes)] for i in range(n_features)]) # Shape: n_features, n_classes.\n",
    "\n",
    "    # Visualization.\n",
    "    if debug_visualization:\n",
    "        counts, bins = np.histogram(np.mean(consistency, axis=1))\n",
    "        plt.hist(bins[:-1], bins, weights=counts)\n",
    "        plt.title(\"Consistency (Averaged Over Classes, Histogram)\")\n",
    "        plt.show()\n",
    "\n",
    "    # 3. Train updated classifiers.\n",
    "    # We train multiple classifiers where we eliminate more and more potentially\n",
    "    # spurious features. Afterwards, we select the best of those classifiers.\n",
    "    classifiers, masks = [], []\n",
    "    n_elimination_candidates = np.linspace(1, n_features, num=100, endpoint=False, dtype=int)\n",
    "    for n_eliminate_per_class in tqdm(n_elimination_candidates, desc=f\"EvA: Training classifier heads with eliminated spurious features (for base model: {base_model})\"): # range(1, int(0.5*n_features))):\n",
    "        # Eliminate top n_eliminate_per_class features per class.\n",
    "        # From each class, we eliminate the feature dimensions with the lowest\n",
    "        # consistency. If multiple classes vote for the same feature dimension\n",
    "        # to be removed, fewer feature dimensions are removed in total.\n",
    "        assert n_eliminate_per_class < n_features, \"At least one feature has to remain after erasing spurious features!\"\n",
    "        assert n_eliminate_per_class > 0, \"Remove at least one feature dimension (otherwise, all would be removed)\"\n",
    "\n",
    "        indices_to_eliminate = np.argsort(consistency, axis=0)[:n_eliminate_per_class, :] # Argsort shape: [n_feature_dims, n_classes].\n",
    "        indices_to_eliminate = np.unique(indices_to_eliminate) # Remove duplicates and flatten.\n",
    "\n",
    "        # indices_to_eliminate = np.unique([np.where(c <= -0.1) for c in consistency.T])\n",
    "\n",
    "        # If we would have to remove all features, this would not really make\n",
    "        # sense. We can apply \"break\" because all subsequent\n",
    "        # runs will fail as well.\n",
    "        if len(indices_to_eliminate) >= n_features:\n",
    "            break\n",
    "\n",
    "        mask=np.full(n_features, True, dtype=bool)\n",
    "        mask[indices_to_eliminate] = False\n",
    "\n",
    "        # Update input data accordingly.\n",
    "        X_biased_eliminated = X_biased[:,mask]\n",
    "        X_unbiased_eliminated = X_unbiased[:,mask]\n",
    "\n",
    "        # Train updated classifier.\n",
    "        # Note that this is essentially a logistic regression. By default, l2\n",
    "        # regularization (weight decay) is applied.\n",
    "        clf = SGDClassifier(\n",
    "            loss=\"log_loss\",\n",
    "            learning_rate=\"constant\", # Scheduler.\n",
    "            eta0=0.001, # Learning rate.\n",
    "            class_weight=\"balanced\",\n",
    "            shuffle=True, # Shuffle data after every epoch.\n",
    "            n_jobs = -1,\n",
    "            random_state=seed,\n",
    "        ).fit(X_biased_eliminated, y_biased)\n",
    "\n",
    "        # print(y_biased.shape)\n",
    "        # print(\"Accuracy on training data:\", str(accuracy_score(y_biased, clf.predict(X_biased_eliminated))))\n",
    "        classifiers.append(clf)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    # 4. Select best-performing classifier on the unbiased dataset.\n",
    "    # Maybe also select following worst-group accuracy since we have access to groups?\n",
    "    accuracies = np.array([accuracy_score(y_true=y_unbiased, y_pred=clf.predict(X_unbiased[:,mask])) for clf, mask in zip(classifiers, masks)])\n",
    "    best_classifier = classifiers[np.argmax(accuracies)]\n",
    "    best_classifier_mask = masks[np.argmax(accuracies)]\n",
    "\n",
    "    # Visualization.\n",
    "    if debug_visualization:\n",
    "        plt.plot(n_elimination_candidates[:len(accuracies)], accuracies) # Adjust x for skipped runs where all features would have been removed.\n",
    "        plt.xlabel(\"Number of Eliminated Feature Dimensions per Class\")\n",
    "        plt.ylabel(\"Worst-Subgroup Accuracy\")\n",
    "        plt.title(\"Classifier Selection\")\n",
    "        plt.show()\n",
    "\n",
    "    # 5. Evaluate.\n",
    "    predictions_test = best_classifier.predict(X_test[:, best_classifier_mask])\n",
    "    eva_predictions.append(predictions_test)\n",
    "    \n",
    "    results[base_model][\"eva_model\"] = np.array([accuracy_score(y_true=y_test[indices], y_pred=predictions_test[indices]) for indices in subset_indices_per_balancing_factor_test])\n",
    "    results[base_model][\"eva_model_balanced\"] = np.array([balanced_accuracy_score(y_true=y_test[indices], y_pred=predictions_test[indices]) for indices in subset_indices_per_balancing_factor_test])\n",
    "    results[base_model][\"eva_model_wsg\"] = np.array([worst_subgroup_accuracy(y_true=y_test[indices], y_pred=predictions_test[indices], group_ids=group_ids_test[indices]) for indices in subset_indices_per_balancing_factor_test])\n",
    "eva_predictions = np.array(eva_predictions)\n",
    "\n",
    "print(\"Training was performed with a biased dataset with \" + str(len(X_biased)) + \" samples and an unbiased dataset with \" + str(len(X_unbiased)) + \" samples each.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Performance on Varying Test Distributions\n",
    "\n",
    "The following plots show the core results of our experiments. We plot the performance (Accuracy, Balanced Accuracy, and Worst-Group Accuracy) of AMSEL and the baseline methods across the full range of simulated test distributions, from highly biased ($\\theta=0$) to fully unbiased ($\\theta=1$).\n",
    "\n",
    "The results demonstrate that while fixed methods like ERM and DFR excel at opposite ends of the spectrum, AMSEL successfully adapts, maintaining high performance across all levels of spurious correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = {\n",
    "    \"Accuracy\": \"\",\n",
    "    \"Balanced Accuracy\": \"_balanced\",\n",
    "    \"Worst-Group Accuracy\": \"_wsg\",\n",
    "}\n",
    "metrics = list(suffixes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    suffix = suffixes[metric]\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 1. Load evaluation results.\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Extract results without aggregation.\n",
    "    # Shape of the results is [n_base_models, n_balancing_parameters (test\n",
    "    # distributions)].\n",
    "    erm_performance = np.array([results[base_model][\"erm_model\" + suffix] for base_model in base_models])\n",
    "    dfr_performance = np.array([results[base_model][\"dfr_model\" + suffix] for base_model in base_models])\n",
    "    eva_performance = np.array([results[base_model][\"eva_model\" + suffix] for base_model in base_models])\n",
    "    amsel_performance = np.array([results[base_model][\"amsel_default_\" + model_selection_strategy + suffix] for base_model in base_models])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. Plot with standard deviations.\n",
    "    # ==========================================================================\n",
    "\n",
    "    plt.figure(figsize=(7.0, 3.0))\n",
    "\n",
    "    # ERM Model.\n",
    "    mean, std = np.mean(erm_performance, axis=0), np.std(erm_performance, axis=0)\n",
    "    plt.plot(\n",
    "        balancing_factors,\n",
    "        mean,\n",
    "        label=\"ERM Model\",\n",
    "        color=colors[\"ERM\"][0],\n",
    "        linestyle=colors[\"ERM\"][2],\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        balancing_factors,\n",
    "        mean+std,\n",
    "        mean-std,\n",
    "        facecolor=colors[\"ERM\"][0],\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    # DFR Model.\n",
    "    mean, std = np.mean(dfr_performance, axis=0), np.std(dfr_performance, axis=0)\n",
    "    plt.plot(\n",
    "        balancing_factors,\n",
    "        mean,\n",
    "        label=\"DFR Model\",\n",
    "        color=colors[\"DFR\"][0],\n",
    "        linestyle=colors[\"DFR\"][2],\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        balancing_factors,\n",
    "        mean+std,\n",
    "        mean-std,\n",
    "        facecolor=colors[\"DFR\"][0],\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    # EvA (Eliminating Spurious Correlations with Activations).\n",
    "    mean, std = np.mean(eva_performance, axis=0), np.std(eva_performance, axis=0)\n",
    "    plt.plot(\n",
    "        balancing_factors,\n",
    "        mean,\n",
    "        label=\"EvA Model\",\n",
    "        color=colors[\"EvA\"][0],\n",
    "        linestyle=colors[\"EvA\"][2],\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        balancing_factors,\n",
    "        mean+std,\n",
    "        mean-std,\n",
    "        facecolor=colors[\"EvA\"][0],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "    # AMSEL (Ours).\n",
    "    mean, std = np.mean(amsel_performance, axis=0), np.std(amsel_performance, axis=0)\n",
    "    plt.plot(\n",
    "        balancing_factors,\n",
    "        mean,\n",
    "        label=\"AMSEL (Ours)\",\n",
    "        color=colors[\"AMSEL (Ours)\"][0],\n",
    "        linestyle=colors[\"AMSEL (Ours)\"][2],\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        balancing_factors,\n",
    "        mean+std,\n",
    "        mean-std,\n",
    "        facecolor=colors[\"AMSEL (Ours)\"][0],\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    # Axis labels.\n",
    "    plt.xlabel(r\"Balancing Parameter $\\theta$ of the Test Distribution\", labelpad=20) # Because we add the second row of the xticks manually\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add custom tick description.\n",
    "    add_tick_descriptions(\n",
    "        fig,\n",
    "        plt.gca(),\n",
    "        balancing_factors,\n",
    "        descs=[\"(Orig. Dist.)\", \"(Unbiased Dist.)\"],\n",
    "        vertical_offset_factor=1.5,\n",
    "        horizontal_offset=28,\n",
    "    )\n",
    "\n",
    "    # Small adjustments for paper displayment.\n",
    "    # For CelebA, make x-ticks and x-label invisible because the accuracy\n",
    "    # plots are above the worst-group accuracy plots and these two share the\n",
    "    # x-axis labeling.\n",
    "    # Note that we cannot disable them because that would change the spacing\n",
    "    # (especially since the x-ticks extend the margin to the right).\n",
    "    if dataset_name == \"celeba\":\n",
    "        plt.gca().xaxis.label.set_color(\"white\")\n",
    "        plt.gca().tick_params(axis='x', colors=\"white\")\n",
    "\n",
    "    # Custom legend.\n",
    "    # Make sure that this is called after tight_layout().\n",
    "    handles = [Line2D([0], [0], label=name, color=color[0], linestyle=color[2]) for name, color in colors.items() if not name == \"Approximated AMSEL (Ours)\"]\n",
    "    plt.legend(\n",
    "        handles=handles,\n",
    "        loc='lower center',\n",
    "        bbox_to_anchor=(0.45, -0.63), # bbox_to_anchor=(1, 0.5),\n",
    "        prop={'size': 10.5},\n",
    "        ncol=len(handles)\n",
    "    )\n",
    "\n",
    "    plt.figtext(\n",
    "        x=0.03,\n",
    "        y=-0.23,\n",
    "        s=(\"Experiment: \" + dataset_name + \" - \" + model_selection_strategy + \"\\n\" + score_function_name + \" - \" + metric),\n",
    "        horizontalalignment='left',\n",
    "        style=\"italic\"\n",
    "    )\n",
    "\n",
    "    plt.savefig(join(output_dir, dataset_name + \" - \" + model_selection_strategy + \" - \" + score_function_name + \" - \" + metric + \" - \" + \" - lineplot.pdf\"), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    suffix = suffixes[metric]\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 1. Load evaluation results.\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Extract means and standard deviations.\n",
    "    erm_performance = np.mean([results[base_model][\"erm_model\" + suffix] for base_model in base_models], axis=0)\n",
    "    dfr_performance = np.mean([results[base_model][\"dfr_model\" + suffix] for base_model in base_models], axis=0)\n",
    "    eva_performance = np.mean([results[base_model][\"eva_model\" + suffix] for base_model in base_models], axis=0)\n",
    "    amsel_performance = np.mean([results[base_model][\"amsel_default_\" + model_selection_strategy + suffix] for base_model in base_models], axis=0)\n",
    "\n",
    "    erm_performance_std = np.std([results[base_model][\"erm_model\" + suffix ] for base_model in base_models], axis=0)\n",
    "    dfr_performance_std = np.std([results[base_model][\"dfr_model\" + suffix] for base_model in base_models], axis=0)\n",
    "    eva_performance_std = np.std([results[base_model][\"eva_model\" + suffix] for base_model in base_models], axis=0)\n",
    "    amsel_performance_std = np.std([results[base_model][\"amsel_default_\" + model_selection_strategy + suffix] for base_model in base_models], axis=0)\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. Export to LaTeX.\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Output LaTeX.\n",
    "    n_skip = 1\n",
    "    index = balancing_factors[::n_skip].tolist()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Balancing Factor\": index,\n",
    "            \"ERM Model\": erm_performance[::n_skip],\n",
    "            \"DFR Model\": dfr_performance[::n_skip],\n",
    "            \"EvA Model\": eva_performance[::n_skip],\n",
    "            \"AMSEL (Ours)\": amsel_performance[::n_skip],\n",
    "            \"ERM Model (Std)\": erm_performance_std[::n_skip],\n",
    "            \"DFR Model (Std)\": dfr_performance_std[::n_skip],\n",
    "            \"EvA Model (Std)\": eva_performance_std[::n_skip],\n",
    "            \"AMSEL (Ours) (Std)\": amsel_performance_std[::n_skip],\n",
    "        }\n",
    "    )\n",
    "    display_df(df)\n",
    "\n",
    "    # Find max values per row.\n",
    "    # Here, we only consider the first 4 digits (i.e., percentages + 2 decimal\n",
    "    # points).\n",
    "    performance_cols = [\"ERM Model\", \"DFR Model\", \"EvA Model\", \"AMSEL (Ours)\"]\n",
    "    df[\"Max Value\"] = df[performance_cols].round(4).max(axis=1)\n",
    "\n",
    "    # Format table with bold max values per row\n",
    "    def format_value(row, col):\n",
    "        value = row[col]\n",
    "        formatted_value = f\"{value:>6.2%}\".replace(\" \", \"\\\\phantom{0}\")\n",
    "\n",
    "        std_col = col + \" (Std)\"\n",
    "        std_value = f\"{row[std_col]:>6.2%}\".replace(\" \", \"\\\\phantom{0}\")\n",
    "        \n",
    "        formatted_value = f\"{formatted_value} ± {std_value}\"\n",
    "\n",
    "        # Bold if it's the max in the row\n",
    "        if value.round(4) == row[\"Max Value\"]:\n",
    "            formatted_value = f\"\\\\textbf{{{formatted_value}}}\"\n",
    "        \n",
    "        return formatted_value\n",
    "\n",
    "    df_formatted = pd.DataFrame(\n",
    "        {\n",
    "            r\"Balancing Parameter $\\theta$\": df[\"Balancing Factor\"].apply(lambda x: f\"{x:.2f}\"),\n",
    "            \"ERM Model\": df.apply(lambda row: format_value(row, \"ERM Model\"), axis=1),\n",
    "            \"DFR Model\": df.apply(lambda row: format_value(row, \"DFR Model\"), axis=1),\n",
    "            \"EvA Model\": df.apply(lambda row: format_value(row, \"EvA Model\"), axis=1),\n",
    "            \"AMSEL (Ours)\": df.apply(lambda row: format_value(row, \"AMSEL (Ours)\"), axis=1),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Remove helper column\n",
    "    df_formatted.drop(columns=[\"Max Value\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Convert to LaTeX\n",
    "    print(\"Evaluation results for metric: \" + metric)\n",
    "    print(df_formatted.to_latex(index=False).replace(\"%\", \"\\\\%\"))\n",
    "\n",
    "    # Export.\n",
    "    with open(join(\".\", \"outputs\", dataset_name, dataset_name + \" - \" + model_selection_strategy + \" - \" + score_function_name + \" - \" + metric + \" on test subset.tex\"), \"w\") as f:\n",
    "        f.write(df_formatted.to_latex(index=False).replace(\"%\", \"\\\\%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a single-number summary of overall performance, we calculate the Area Under the Curve (AUC) for the accuracy plots. A higher AUC indicates better average performance across the entire spectrum of test distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider accuracy.\n",
    "suffix = \"\"\n",
    "\n",
    "# Extract results without aggregation.\n",
    "# Shape of the results is [n_base_models, n_balancing_parameters (test\n",
    "# distributions)].\n",
    "erm_performance = np.array([results[base_model][\"erm_model\" + suffix] for base_model in base_models])\n",
    "dfr_performance = np.array([results[base_model][\"dfr_model\" + suffix] for base_model in base_models])\n",
    "eva_performance = np.array([results[base_model][\"eva_model\" + suffix] for base_model in base_models])\n",
    "amsel_performance = np.array([results[base_model][\"amsel_default_\" + model_selection_strategy + suffix] for base_model in base_models])\n",
    "\n",
    "# Calculate AuCs.\n",
    "area_under_the_curve = {\n",
    "    \"Model\": [\"ERM Model\", \"DFR Model\", \"EvA Model\", \"AMSEL (Ours)\"],\n",
    "    \"AuC\": [],\n",
    "}\n",
    "for performance in [erm_performance, dfr_performance, eva_performance, amsel_performance]:\n",
    "    aucs = [auc(balancing_factors, p) for p in performance]\n",
    "    area_under_the_curve[\"AuC\"].append(f\"${np.mean(aucs):.3f} \\\\pm {np.std(aucs): .3f}$\")\n",
    "\n",
    "# Highlight maximum element.\n",
    "max_index = np.argmax(area_under_the_curve[\"AuC\"])\n",
    "area_under_the_curve[\"AuC\"][max_index] = r\"\\textbf{\" + area_under_the_curve[\"AuC\"][max_index] + \"}\"\n",
    "\n",
    "area_under_the_curve = pd.DataFrame(area_under_the_curve)\n",
    "display_df(area_under_the_curve)\n",
    "print(area_under_the_curve.to_latex(index=False))\n",
    "\n",
    "# # Export.\n",
    "with open(join(output_dir, dataset_name + \" - \" + model_selection_strategy + \" - \" + score_function_name + \" - AuC Accuracy.tex\"), \"w\") as f:\n",
    "    f.write(area_under_the_curve.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Accuracy Trade-off Analysis\n",
    "\n",
    "This plot visualizes the trade-off between performance on the original biased test distribution ($\\theta=0$) and a fully unbiased one ($\\theta=1$). Each point on the green line represents one of our candidate models, $m_\\theta$. The line traces the Pareto front of models optimized for different bias levels. Fixed methods like ERM and DFR occupy single points in this space. AMSEL's strength lies in its ability to dynamically select an appropriate model from this front based on the test data, thereby navigating the trade-off effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup.\n",
    "# In order to evaluate the methods on a balanced test distribution, we first\n",
    "# have to extract the corresponding indices w.r.t. to the original test dataset.\n",
    "indices_unbiased_testset = subset_indices_per_balancing_factor_test[-1]\n",
    "\n",
    "plt.figure(figsize=(7.0,2.8))\n",
    "# plt.figure(figsize=(7.5,3.5))\n",
    "\n",
    "# ERM model.\n",
    "# erm_accuracy_unbiased = np.mean([accuracy_score(labels_test[indices_unbiased_testset], p[indices_unbiased_testset]) for p in erm_predictions], axis=0)\n",
    "# erm_accuracy_biased = np.mean([accuracy_score(labels_test, p) for p in erm_predictions], axis=0)\n",
    "\n",
    "# plt.scatter(\n",
    "#     erm_accuracy_unbiased,\n",
    "#     erm_accuracy_biased,\n",
    "#     marker=\"D\",\n",
    "#     s=50,\n",
    "#     color=colors[\"ERM\"][0],\n",
    "#     label=\"ERM Model\",\n",
    "#     zorder=1,\n",
    "# )\n",
    "\n",
    "# DFR model.\n",
    "dfr_accuracy_unbiased = np.mean([accuracy_score(labels_test[indices_unbiased_testset], p[indices_unbiased_testset]) for p in dfr_predictions[:,:,unbiased_model_idx]], axis=0)\n",
    "dfr_accuracy_biased = np.mean([accuracy_score(labels_test, p) for p in dfr_predictions[:,:,unbiased_model_idx]], axis=0)\n",
    "\n",
    "plt.scatter(\n",
    "    dfr_accuracy_unbiased,\n",
    "    dfr_accuracy_biased,\n",
    "    marker=\"D\",\n",
    "    s=50,\n",
    "    color=colors[\"DFR\"][0],\n",
    "    label=\"DFR Model\",\n",
    "    zorder=4,\n",
    ")\n",
    "\n",
    "# Retrained candidate classifiers.\n",
    "candidate_classifiers_accuracy_unbiased = np.mean(\n",
    "    np.array(\n",
    "        [\n",
    "            [accuracy_score(labels_test[indices_unbiased_testset], p[indices_unbiased_testset]) for p in predictions.T]\n",
    "            for predictions in dfr_predictions\n",
    "        ]\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "candidate_classifiers_accuracy_biased = np.mean(\n",
    "    np.array(\n",
    "        [\n",
    "            [accuracy_score(labels_test, p) for p in predictions.T]\n",
    "            for predictions in dfr_predictions\n",
    "        ]\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    candidate_classifiers_accuracy_unbiased,\n",
    "    candidate_classifiers_accuracy_biased,\n",
    "    label=\"Candidate Classifiers\",\n",
    "    marker=\"o\",\n",
    "    color=colors[\"AMSEL (Ours)\"][0],\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Accuracy (Unbiased)\")\n",
    "plt.ylabel(\"Accuracy (Biased)\")\n",
    "# plt.title(dataset_name + \" - Trade-Off Between Accuracy on the Biased and Unbiased Test Dataset\\n- on test subset\", pad=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Legend.\n",
    "# Make sure that this is called after tight_layout().\n",
    "plt.legend(\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.45, -0.65), # bbox_to_anchor=(1, 0.5),\n",
    "    prop={'size': 10.5},\n",
    "    ncol=len(handles)\n",
    ")\n",
    "\n",
    "plt.figtext(\n",
    "    x=0.03,\n",
    "    y=-0.4,\n",
    "    s=(\"Experiment: \" + dataset_name + \" - Trade-Off Between\\nAccuracy on the Biased and Unbiased\\nTest Dataset - on test subset\"),\n",
    "    horizontalalignment='left',\n",
    "    style=\"italic\"\n",
    ")\n",
    "\n",
    "plt.savefig(join(output_dir, dataset_name + \" - Trade-Off Between Accuracy on the Biased and Unbiased Test Dataset\" \" - \" + \".pdf\"), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amsel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
